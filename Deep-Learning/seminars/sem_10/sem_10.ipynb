{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Verteeva_RNN_Exercise_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Character-Level LSTM in PyTorch\n",
        "\n",
        "На этом семинаре поговорим про RNN. Здесь мы обучим модель на тексте Анны Карениной, а затем будем генерировать новый текст.**Эта модель сможет генерировать новый текст на основе нашего текста!**\n",
        "\n",
        "Можно посомтреть [статья об RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) и [реализация в Torch](https://github.com/karpathy/char-rnn). Ниже представлена ​​общая архитектура RNN.\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMb9A1BIceEl"
      },
      "source": [
        "Сначала загрузим ресурсы, необходимые для загрузки данных и создания модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "Затем мы загрузим текстовый файл Анны Карениной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/khaykingleb/Deep-Learning/master/seminars/sem_10/anna.txt'\n",
        "r = requests.get(url)\n",
        "\n",
        "# make sure your filename is the same as how you want to import \n",
        "with open('anna.txt', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# open text file and read in data as `text`\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      },
      "source": [
        "Давайте проверим первые 100 символов, убедимся, что все красиво. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VctmLQfceEl",
        "outputId": "b9ef58da-0fd3-4603-c6e7-2804af6daebf"
      },
      "source": [
        "print(text[:144])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everything was in confusion in the Oblonskys' house. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iC21bopceEl"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "В ячейках ниже постараемся создать пару **словарей** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVlmnxLceEl"
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sdeHMT0Uxmd",
        "outputId": "7c1f3c3e-7c76-412b-d95a-5605599632a6"
      },
      "source": [
        "char2int"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 53,\n",
              " ' ': 73,\n",
              " '!': 11,\n",
              " '\"': 55,\n",
              " '$': 2,\n",
              " '%': 65,\n",
              " '&': 42,\n",
              " \"'\": 30,\n",
              " '(': 0,\n",
              " ')': 41,\n",
              " '*': 46,\n",
              " ',': 28,\n",
              " '-': 29,\n",
              " '.': 82,\n",
              " '/': 76,\n",
              " '0': 18,\n",
              " '1': 22,\n",
              " '2': 6,\n",
              " '3': 21,\n",
              " '4': 71,\n",
              " '5': 3,\n",
              " '6': 16,\n",
              " '7': 5,\n",
              " '8': 19,\n",
              " '9': 47,\n",
              " ':': 56,\n",
              " ';': 14,\n",
              " '?': 43,\n",
              " '@': 66,\n",
              " 'A': 8,\n",
              " 'B': 79,\n",
              " 'C': 68,\n",
              " 'D': 1,\n",
              " 'E': 25,\n",
              " 'F': 23,\n",
              " 'G': 67,\n",
              " 'H': 59,\n",
              " 'I': 51,\n",
              " 'J': 44,\n",
              " 'K': 75,\n",
              " 'L': 48,\n",
              " 'M': 50,\n",
              " 'N': 39,\n",
              " 'O': 34,\n",
              " 'P': 63,\n",
              " 'Q': 64,\n",
              " 'R': 58,\n",
              " 'S': 38,\n",
              " 'T': 33,\n",
              " 'U': 17,\n",
              " 'V': 27,\n",
              " 'W': 61,\n",
              " 'X': 24,\n",
              " 'Y': 69,\n",
              " 'Z': 62,\n",
              " '_': 7,\n",
              " '`': 35,\n",
              " 'a': 70,\n",
              " 'b': 57,\n",
              " 'c': 36,\n",
              " 'd': 72,\n",
              " 'e': 52,\n",
              " 'f': 12,\n",
              " 'g': 13,\n",
              " 'h': 74,\n",
              " 'i': 77,\n",
              " 'j': 80,\n",
              " 'k': 81,\n",
              " 'l': 31,\n",
              " 'm': 78,\n",
              " 'n': 54,\n",
              " 'o': 40,\n",
              " 'p': 37,\n",
              " 'q': 15,\n",
              " 'r': 60,\n",
              " 's': 45,\n",
              " 't': 20,\n",
              " 'u': 49,\n",
              " 'v': 32,\n",
              " 'w': 26,\n",
              " 'x': 9,\n",
              " 'y': 4,\n",
              " 'z': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJIzwzSwceEl"
      },
      "source": [
        "И мы можем видеть те же самые символы сверху, закодированные как целые числа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK1MYr_9ceEl",
        "outputId": "d5bf87c1-4527-4de6-f789-9ccd287764d4"
      },
      "source": [
        "# последовательность закодированных символов\n",
        "encoded[:100]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([68, 74, 70, 37, 20, 52, 60, 73, 22, 53, 53, 53, 59, 70, 37, 37,  4,\n",
              "       73, 12, 70, 78, 77, 31, 77, 52, 45, 73, 70, 60, 52, 73, 70, 31, 31,\n",
              "       73, 70, 31, 77, 81, 52, 14, 73, 52, 32, 52, 60,  4, 73, 49, 54, 74,\n",
              "       70, 37, 37,  4, 73, 12, 70, 78, 77, 31,  4, 73, 77, 45, 73, 49, 54,\n",
              "       74, 70, 37, 37,  4, 73, 77, 54, 73, 77, 20, 45, 73, 40, 26, 54, 53,\n",
              "       26, 70,  4, 82, 53, 53, 25, 32, 52, 60,  4, 20, 74, 77, 54])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azltQy-gceEl"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "Как вы можете видеть на изображении char-RNN выше, наш LSTM ожидает ввода, который  **one-hot encoded** , что означает, что каждый символ преобразуется в целое число (через наш созданный словарь), а затем преобразуется в столбец вектор, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnahALhiceEl"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3lTdLKfceEl",
        "outputId": "5f84cd38-4e2c-4729-f746-90d880c828d2"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YyL91CuceEl"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "\n",
        "Для обучения на этих данных нужно создать мини-батчи для обучения. На простом примере наши батчи будут выглядеть так:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "В этом примере возьмем закодированные символы (переданные как параметр arr) и разделим их на несколько последовательностей, заданных параметром batch_size. Каждая из наших последовательностей будет иметь длину seq_length.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи.**\n",
        "\n",
        "Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина seq_length или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива arr, нужно разделить длину arr на количество символов в батче. Как только вы знаете количество пакетов, вы можете получить общее количество символов, которые нужно сохранить, из `arr`, $ N * M * K $.\n",
        "\n",
        "**2. После этого нам нужно разделить arr на $N$ пакетов.** \n",
        "\n",
        "Вы можете сделать это с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n",
        "\n",
        "**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи.**\n",
        "\n",
        "Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на seq_length. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину seq_length.\n",
        "\n",
        "> **TODO:** Напишите код для создания батчей в функции ниже. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vmDKLiOceEl"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence '''\n",
        "    \n",
        "    # Общее количество символов в батче \n",
        "    batch_size_total = batch_size * seq_length\n",
        "\n",
        "    # Get the number of batches we can make\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:batch_size_total * n_batches]\n",
        "    \n",
        "    # Reshape into batch_size rows\n",
        "    arr = np.reshape(np.array(arr), (batch_size, -1))\n",
        "    \n",
        "    # Iterate over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n: n + seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = arr[:, (n + 1): (n + 1) + seq_length]\n",
        "        # Применяем, если индекс не выходит за границы последловательности\n",
        "        if (n + 1) + seq_length <= arr.shape[1]:\n",
        "            yield x, y"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9uKOvbqceEl"
      },
      "source": [
        "### Test Your Implementation\n",
        "\n",
        "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы пакетируем данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtKlLXi1ceEl"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg5MUTqqceEl",
        "outputId": "e81258f6-d35c-4db8-b9b4-c9421a3f1fb1"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[68 74 70 37 20 52 60 73 22 53]\n",
            " [45 40 54 73 20 74 70 20 73 70]\n",
            " [52 54 72 73 40 60 73 70 73 12]\n",
            " [45 73 20 74 52 73 36 74 77 52]\n",
            " [73 45 70 26 73 74 52 60 73 20]\n",
            " [36 49 45 45 77 40 54 73 70 54]\n",
            " [73  8 54 54 70 73 74 70 72 73]\n",
            " [34 57 31 40 54 45 81  4 82 73]]\n",
            "\n",
            "y\n",
            " [[74 70 37 20 52 60 73 22 53 53]\n",
            " [40 54 73 20 74 70 20 73 70 20]\n",
            " [54 72 73 40 60 73 70 73 12 40]\n",
            " [73 20 74 52 73 36 74 77 52 12]\n",
            " [45 70 26 73 74 52 60 73 20 52]\n",
            " [49 45 45 77 40 54 73 70 54 72]\n",
            " [ 8 54 54 70 73 74 70 72 73 45]\n",
            " [57 31 40 54 45 81  4 82 73 55]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qHIAEIceEl"
      },
      "source": [
        "Если вы правильно реализовали get_batches, результат должен выглядеть примерно так:\n",
        "```\n",
        "x\n",
        " [[25  8 60 11 45 27 28 73  1  2]\n",
        " [17  7 20 73 45  8 60 45 73 60]\n",
        " [27 20 80 73  7 28 73 60 73 65]\n",
        " [17 73 45  8 27 73 66  8 46 27]\n",
        " [73 17 60 12 73  8 27 28 73 45]\n",
        " [66 64 17 17 46  7 20 73 60 20]\n",
        " [73 76 20 20 60 73  8 60 80 73]\n",
        " [47 35 43  7 20 17 24 50 37 73]]\n",
        "\n",
        "y\n",
        " [[ 8 60 11 45 27 28 73  1  2  2]\n",
        " [ 7 20 73 45  8 60 45 73 60 45]\n",
        " [20 80 73  7 28 73 60 73 65  7]\n",
        " [73 45  8 27 73 66  8 46 27 65]\n",
        " [17 60 12 73  8 27 28 73 45 27]\n",
        " [64 17 17 46  7 20 73 60 20 80]\n",
        " [76 20 20 60 73  8 60 80 73 17]\n",
        " [35 43  7 20 17 24 50 37 73 36]]\n",
        " ```\n",
        " хотя точные цифры могут отличаться. Убедитесь, что данные сдвинуты на один шаг для `y`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jouxv0L2ceEl"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>\n",
        "\n",
        "Затем используем PyTorch для определения архитектуры сети. Начнем с определения слоев и операций, методов прямого прохода. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7s5eRaoceEl"
      },
      "source": [
        "### Model Structure\n",
        "\n",
        "В `__init__` предлагаемая структура выглядит следующим образом:\n",
        "* Создавать и хранить необходимые словари (это было сделано за вас)\n",
        "* Определите слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность выпадения `drop_prob` и логическое значение `batch_first` (True)\n",
        "* Определите слой отброса данных с помощью drop_prob\n",
        "* Определите полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода — количество символов\n",
        "* Наконец, инициализируйте веса\n",
        "\n",
        "Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде `self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm1atCuceEl"
      },
      "source": [
        "---\n",
        "### LSTM Inputs/Outputs\n",
        "\n",
        "Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) следующим образом:\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "где `input_size` — это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidden` — это количество единиц в скрытых слоях ячейки. Можно добавить выпадение с заданной вероятностью; это автоматически добавит отсев на входах или выходах. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`. При этом он отправляет вывод одной ячейки в следующую ячейку.\n",
        "\n",
        "Здесь же требуется создать начальное скрытое состояние всех нулей:\n",
        "\n",
        "```python\n",
        "self.init_hidden()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlTnDntHceEl",
        "outputId": "16333eb8-e531-4729-d15e-3351408eb741"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg-SvaGhceEl"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the layers of the model\n",
        "        self.lstm = nn.LSTM(len(tokens), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        self.fc = nn.Linear(n_hidden, len(tokens))\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        out_lstm, _ = self.lstm(x, hidden)\n",
        "        out_lstm = out_lstm.contiguous().view((-1, self.n_hidden))\n",
        "        out = self.fc(out_lstm)\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrBRlEPceEl"
      },
      "source": [
        "## Time to train\n",
        "\n",
        "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
        "\n",
        "Используем оптимизатор Адама и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation!\n",
        "\n",
        "Пара подробностей об обучении:\n",
        "> * В рамках цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что LSTM имеет скрытое состояние, которое является кортежем скрытых состояний.\n",
        "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающегося градиента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv8VkRI0ceEl"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.reshape(batch_size*seq_length).long()) ### view if train_on_gpu\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.reshape(batch_size*seq_length).long()) ### view if train_on_gpu\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                clear_output()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0q4KGEceEm"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOgs59nDceEm",
        "outputId": "d7564ccf-3c2f-4863-e040-6aed602b2f96"
      },
      "source": [
        "## TODO: set your model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden = 100\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 100, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=100, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy6mECuceEm"
      },
      "source": [
        "### Set your training hyperparameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABqi9klKceEm",
        "scrolled": true,
        "outputId": "aa81fc15-7b30-431a-c1c9-0cfcc0e8e1d2"
      },
      "source": [
        "batch_size = 32\n",
        "seq_length = 32\n",
        "n_epochs =  10\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/10... Step: 17430... Loss: 1.5798... Val Loss: 1.6489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wApq1rceEm"
      },
      "source": [
        "## Getting the best model\n",
        "\n",
        "Чтобы настроить гиперпараметры на максимальную производительность, вам нужно будет посмотреть потери при обучении и проверке. Если ваша потеря на обучении намного ниже, чем потеря на тесте, то модель переобучена. Увеличьте регуляризацию или уменьшите число слоев в сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWojDegbsvWK"
      },
      "source": [
        "В ячейке ниже вы можете увидеть рекомендации, которые были найдены в статьях по оптимизации RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH_QUEmbceEm"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Гиперпараметры сети:\n",
        "\n",
        "* `n_hidden` - Количество юнитов в скрытых слоях.\n",
        "* `n_layers` - Количество используемых скрытых слоев LSTM.\n",
        "\n",
        "В нашем примере вероятность отсева и скорость обучения сохраняется.\n",
        "\n",
        "Для обучения:\n",
        "* `batch_size` - количество объектов в батче, проходящих по сети за один проход.\n",
        "* `seq_length` - Количество символов в последовательности, на которой обучается сеть. Обычно чем больше, тем лучше, сеть будет изучать более дальние зависимости.\n",
        "* `lr` - learning rate.\n",
        "\n",
        "\n",
        " ## Советы и хитрости\n",
        "\n",
        "> -  В глубоком обучении очень распространено запускать множество различных моделей с множеством различных настроек гиперпараметров и, в конце концов, использовать любую контрольную точку, дающую наилучшую производительность проверки.\n",
        "\n",
        "> - Кстати, размер ваших тренировочных и проверочных разделов также является параметрами. Убедитесь, что у вас есть приличный объем данных в вашем наборе проверки, иначе производительность проверки будет шумной и не очень информативной."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZxvNoDceEm"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6RXl5VAceEm"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2sJhx5iceEm"
      },
      "source": [
        "---\n",
        "## Making Predictions\n",
        "\n",
        "Теперь, когда модель обучена, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ обучения, и сеть предсказывает следующий символ, который мы потом передаем обратно и получаем еще один предсказанный символ и так далее...\n",
        "\n",
        "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные символы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/torch.html#torch.topk).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEIRW_B2ceEm"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG38j3gQceEm"
      },
      "source": [
        "### Priming and generating text \n",
        "\n",
        "Нужно создать скрытое состояние, чтобы сеть не генерировала произвольные символы. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vpB5gRceEm"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmFA9eEceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56770700-b887-4640-b0f6-0efa8584d301"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna withastheding tere an he se whith he hes ase he hanondere wime hing sa ser he ser hato thase se st tomarator tin hestor sarero he asede wond at atis wise he a t himong wererindes an s t wathat hatidind ton her an her her athe tind wa wisere he wing a t th astomonthe s t t wed ate at hind thand ha shithes andere st white arastendingis to t s storen than houlle thandond wa womeden ting war wathast tinthan ang and se the s werim s s s the wom t hithan thed s he hout s anoraten anghatinong herer here and angoulido as hintin windind h wering wit ther anonomastondis winthedend asen serouly win whe as herom win ar hend sthinge wan are win he t and a har t andound whe tont t thid a toung atouly t a ton ted andid ared he a wind whomer sand sa he anthes thende s s he s s t ang andonde atimous wathim werererond hate an wathe tonong woulle tes he sthe sthedont wererero th as whinder winde therashinde hatorathind whero t whes she his wanont t se sthe andoreren thisanthasa a wed want ateng s atouthe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942mjdQHceEm"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt9ldUuSceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3b21f6-cee6-4358-98df-2de4304caa11"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut6R3zDcceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4f94eb-03ce-4d25-8e98-1243dc1ec445"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said thount shed atoma athed thenom woute atoure whimereres t wed the tourathe are athenong hand wint wid ang and sand ter hathanthed s thar th a watherand ang a thed hinoureris he tontoun winome astherer santerorin we wasa ande wedomind tit tit s she hererarin st ware s an hared tonge athes as wa t s atis t thaserat we ang ar t wate s as are thong t wheno wise se we wedorar and t harome sthe s winthan wonthare tis t hatound anon hered a wa tomerathere t he tes hise as t ande h whest hintind t andoriminout s the sen heronted thindo ast t that ared ar t atintho her anoma t hatous an whind se arontor hinouto s tenthe s hingid an tere ha timouthomond san hanounonond a the andingheren atount at tistereng tong t wongharin a athare thide wan thated se t anthous wand an sed athere withere hen as horese wing theroulomeses t t ting thinome te and thing s her whin s t torongo a ter ar wo s hat at s we a the ar t s won tha t athis the he tor a the st te as thenomendes and sth ant t antoron atho terid\n",
            "we hes t hitherer are hanout wo ang houte ated an thed thes thended athas he has hit he heras he witout as asaser ate the atere toulis watous he ware the the wo st wheder anous th st teder her withing hes wa toustor asenous wa hithe andisthit thome wat thind we horange hedithise touthere wand wat wan wheshe se thino athenge whe an the atha an atid as hatit angin anding athe arouser sa therin the her thithent ar wang herer wangondist thathe t shondo sthe ang as arater asthare t thed an astithedid wont at her s shed thint t sa t witenghar t ses a t the hestharin a he an wond athin t s s s that wing tor were athin s t thitoust a he thendorom hend the ar s t whingh t hinging a wo a t ange hat thede watin has te t we thon tout tho s ase the thomed the that weng ang wountinorar we t wit hed\n",
            "\"\n",
            "an s ano sere athe as tin tharer an a an sa s a an at thedon a and arenderithare t ang anghe as herat wande he wisar wh s t tid he ang at s t ate ting athen t sat asterindo wheno hit sharere sered s ang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZcFw_5dqjd3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}