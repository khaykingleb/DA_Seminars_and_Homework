{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил: Хайкин Глеб\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Оценка за ДЗ вычисляется по следующей формуле:\n",
    "\n",
    "$$\n",
    "\\min(\\text{points}, 21)  \\times 10 / 21,\n",
    "$$\n",
    "\n",
    "где points — количество баллов за домашнее задание, которое вы набрали. Максимальное число баллов, которое можно получить за решение данного домашнего задания — 24, все баллы сверх 21 идут в бонус (таким образом, за данное домашнее задание можно получить 3 бонусных балла). Накопленные бонусные баллы можно будет потом распределять по другим домашним заданиям и проверочным (+1 бонусный балл = +1 к оценке за домашнее задание/проверочную).\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - `'title'`\n",
    " - `'description'`\n",
    " - `'Category_name'`\n",
    " - `'Category'`\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (`'title'`, `'description'`) предсказать `'Category'`.\n",
    "(Очевидно, что параметр `'Category_name'` для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "b345f049-ae77-4d1b-a25f-4d4f447e63d2",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382220</th>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397529</th>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584569</th>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513100</th>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091886</th>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "id                                   \n",
       "382220                    Прихожая   \n",
       "397529   Кордиант 215/55/16 Летние   \n",
       "584569                        Стол   \n",
       "2513100                 Комбинезон   \n",
       "1091886                   Ветровка   \n",
       "\n",
       "                                               description  \\\n",
       "id                                                           \n",
       "382220                           В хорошем состоянии. Торг   \n",
       "397529   Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...   \n",
       "584569   Стол, 2 рабочих места . Стол серого цвета, в д...   \n",
       "2513100                                       Размер-42/44   \n",
       "1091886                                          На 2 года   \n",
       "\n",
       "                     Category_name  Category  \n",
       "id                                            \n",
       "382220           Мебель и интерьер        20  \n",
       "397529       Запчасти и аксессуары        10  \n",
       "584569           Мебель и интерьер        20  \n",
       "2513100  Одежда, обувь, аксессуары        27  \n",
       "1091886     Детская одежда и обувь        29  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"avito_data.csv\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "96ed00ed-b63b-4478-f2d4-66bda1110b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "fd4d5b55-a023-4129-9ff5-a6e8e24db915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
       "       ['Светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
       "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
       "       ['Пион Ирис Ромашка рассада',\n",
       "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
       "       ['Кофта', 'Состояние отличное']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27,  20,  84, 106,  27])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (0.5 балла)\n",
    "\n",
    "\n",
    "Токенизация — разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации. В данном задании мы будем использовать `WordPunctTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: ['здраствуйте', '.', 'я', ',', 'кирилл', '.', 'хотел', 'бы', 'чтобы', 'вы', 'сделали', 'игру', ',', '3д', '-', 'экшон', 'суть', 'такова', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "\n",
    "print(\"before:\", text)\n",
    "print(\"after:\", tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ реализуйте функцию ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "59ef3a75-008e-47c5-fba8-a319eba13ef4"
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    \n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preprocess(text, tokenizer) == 'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ токенизируйте `'title'` и `'description'` в `train` и `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "X_train = np.array([[preprocess(item[0], tokenizer), preprocess(item[1], tokenizer)] for item in X_train])\n",
    "X_test = np.array([[preprocess(item[0], tokenizer), preprocess(item[1], tokenizer)] for item in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[5][0] == '1 - к квартира , 33 м² , 4 / 5 эт .'\n",
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'\n",
    "assert X_test[2][0] == 'фара правая toyota rav 4 галоген 2015 - 19'\n",
    "assert X_test[2][1] == 'фара правая для toyota rav4 2015 / оригинальный номер : 8113042650 / тойота рав4 тоета рав 4 / производитель : toyota / состояние : отличное без дефектов ! / комментарий : после 2015 не ксенон галоген + диод / пожалуйста , уточняйте соответствие вашего заказа изображенному на фото . / звоните уточняйте по наличию предоставляется время на проверку детали / отправляем в регионы рф транспортными компаниями / . / всегда включен вайбер вацап по вопросам !/ дополнительное фото по запросу'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (3 балла)\n",
    "\n",
    "Один из традиционных подходов — построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в `train data`\n",
    " - Для каждого примера из `train` посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В `sklearn` есть `CountVectorizer`, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ создайте словарь, где каждому токену соответствует количество раз, которое оно встретилось в `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:01<00:00, 14837.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for title, description in tqdm(X_train):\n",
    "    string = title + ' ' + description\n",
    "    for word in string.split():\n",
    "         tokens_cnt[word] = tokens_cnt.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens_cnt['сапоги'] == 454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ выведите 10 самых частотных и 10 самых редких токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(tokens_cnt, key=lambda x: tokens_cnt.get(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 самых частых токенов: ['/', ',', '.', '-', 'в', 'и', 'на', './', ':', 'с']\n"
     ]
    }
   ],
   "source": [
    "print(f'10 самых частых токенов: {sorted_tokens[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 самых редких токенов: ['фрионом', 'хлебозаводская', 'дооснастить', 'беспрецедентно', 'понравившейся', 'объективную', 'столиц', 'петровского', 'гремят', 'шуршат']\n"
     ]
    }
   ],
   "source": [
    "print(f'10 самых редких токенов: {sorted_tokens[::-1][:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ оставьте в словаре только топ-10000 самых частотных токенов, также создайте отдельный список из этих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Оставим только топ-10000 самых частотных токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sorted_tokens[10000:]:\n",
    "    tokens_cnt.pop(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cоздадим отдельный список из этих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " ',',\n",
       " '.',\n",
       " '-',\n",
       " 'в',\n",
       " 'и',\n",
       " 'на',\n",
       " './',\n",
       " ':',\n",
       " 'с',\n",
       " '(',\n",
       " 'по',\n",
       " 'для',\n",
       " 'не',\n",
       " ')',\n",
       " '2',\n",
       " '1',\n",
       " '!',\n",
       " 'до',\n",
       " 'от',\n",
       " '—',\n",
       " '\"',\n",
       " '3',\n",
       " '5',\n",
       " 'состоянии',\n",
       " '!/',\n",
       " 'у',\n",
       " 'за',\n",
       " 'все',\n",
       " 'размер',\n",
       " 'без',\n",
       " 'из',\n",
       " '*',\n",
       " '4',\n",
       " 'есть',\n",
       " 'доставка',\n",
       " '00',\n",
       " 'см',\n",
       " 'к',\n",
       " 'продам',\n",
       " 'состояние',\n",
       " 'или',\n",
       " ')/',\n",
       " '10',\n",
       " 'цена',\n",
       " ':/',\n",
       " 'б',\n",
       " '•',\n",
       " 'наличии',\n",
       " 'руб',\n",
       " '6',\n",
       " 'при',\n",
       " 'очень',\n",
       " 'р',\n",
       " 'фото',\n",
       " 'новые',\n",
       " 'звоните',\n",
       " 'можно',\n",
       " 'м',\n",
       " 'запчасти',\n",
       " ';/',\n",
       " '+',\n",
       " 'новый',\n",
       " '8',\n",
       " 'отличном',\n",
       " '20',\n",
       " '7',\n",
       " 'продаю',\n",
       " 'хорошем',\n",
       " 'гарантия',\n",
       " 'как',\n",
       " 'под',\n",
       " '9',\n",
       " ';',\n",
       " 'а',\n",
       " '),',\n",
       " 'так',\n",
       " '–',\n",
       " 'мы',\n",
       " 'г',\n",
       " '«',\n",
       " 'платье',\n",
       " 'мм',\n",
       " 'работы',\n",
       " 'оригинал',\n",
       " ').',\n",
       " 'квартира',\n",
       " 'россии',\n",
       " 'вы',\n",
       " 'торг',\n",
       " 'цвет',\n",
       " 'комплект',\n",
       " 'м²',\n",
       " 'новая',\n",
       " '50',\n",
       " '↓',\n",
       " '100',\n",
       " 'только',\n",
       " 'более',\n",
       " 'номер',\n",
       " 'шт',\n",
       " '%',\n",
       " 'запчастей',\n",
       " 'регионы',\n",
       " 'лет',\n",
       " '0',\n",
       " 'дом',\n",
       " 'года',\n",
       " '!!!',\n",
       " 'что',\n",
       " 'также',\n",
       " 'вас',\n",
       " 'отличное',\n",
       " 'ул',\n",
       " '15',\n",
       " 'любой',\n",
       " 'работаем',\n",
       " '30',\n",
       " 'длина',\n",
       " 'куртка',\n",
       " '12',\n",
       " 'рублей',\n",
       " 'д',\n",
       " 'же',\n",
       " 'отправка',\n",
       " 'цены',\n",
       " '✔',\n",
       " 'вам',\n",
       " 'пишите',\n",
       " 'время',\n",
       " 'нет',\n",
       " 'раз',\n",
       " 'телефону',\n",
       " 'со',\n",
       " 'нас',\n",
       " '18',\n",
       " 'нашем',\n",
       " 'большой',\n",
       " 'ремонт',\n",
       " 'товара',\n",
       " '40',\n",
       " 'другие',\n",
       " 'возможна',\n",
       " '19',\n",
       " 'авто',\n",
       " 'но',\n",
       " 'х',\n",
       " 'система',\n",
       " 'шины',\n",
       " 'артикул',\n",
       " 'один',\n",
       " 'магазин',\n",
       " '✅',\n",
       " 'т',\n",
       " 'эт',\n",
       " 'уточняйте',\n",
       " 'автомобиль',\n",
       " 'это',\n",
       " 'компаниями',\n",
       " 'год',\n",
       " 'ассортимент',\n",
       " 'если',\n",
       " 'можете',\n",
       " 'рф',\n",
       " '16',\n",
       " 'транспортными',\n",
       " 'день',\n",
       " 'км',\n",
       " '»',\n",
       " 'комбинезон',\n",
       " 'подарок',\n",
       " 'хорошее',\n",
       " '000',\n",
       " 'выбор',\n",
       " 'кг',\n",
       " 'кожа',\n",
       " 'после',\n",
       " 'магазине',\n",
       " '60',\n",
       " 'наш',\n",
       " '.,',\n",
       " 'высота',\n",
       " 'кв',\n",
       " 'размеры',\n",
       " 'полный',\n",
       " ')./',\n",
       " '17',\n",
       " '14',\n",
       " 'оплата',\n",
       " 'обмен',\n",
       " 'авито',\n",
       " 'бесплатно',\n",
       " '500',\n",
       " 'наличие',\n",
       " 'блок',\n",
       " 'костюм',\n",
       " 'детали',\n",
       " 'дома',\n",
       " 'ч',\n",
       " '!!!/',\n",
       " 'бу',\n",
       " 'сайте',\n",
       " 'города',\n",
       " 'москве',\n",
       " 'всегда',\n",
       " 'комплекте',\n",
       " 'модель',\n",
       " 'стоимость',\n",
       " '200',\n",
       " 'туфли',\n",
       " '№',\n",
       " '11',\n",
       " 'рост',\n",
       " 'самовывоз',\n",
       " 's',\n",
       " 'два',\n",
       " 'идеальном',\n",
       " 'метро',\n",
       " 'много',\n",
       " '24',\n",
       " 'цвета',\n",
       " 'возможен',\n",
       " ',/',\n",
       " 'вопросы',\n",
       " 'адрес',\n",
       " '300',\n",
       " 'магазина',\n",
       " 'автомобилей',\n",
       " 'материал',\n",
       " 'задний',\n",
       " 'о',\n",
       " '~',\n",
       " 'автомобиля',\n",
       " 'бампер',\n",
       " 'ботинки',\n",
       " 'оригинальный',\n",
       " 'во',\n",
       " '25',\n",
       " 'новое',\n",
       " 'товар',\n",
       " '44',\n",
       " 'диски',\n",
       " 'рядом',\n",
       " 'через',\n",
       " 'продается',\n",
       " 'качество',\n",
       " '80',\n",
       " 'установка',\n",
       " 'ширина',\n",
       " 'двигатель',\n",
       " 'работает',\n",
       " 'бесплатная',\n",
       " 'mercedes',\n",
       " 'весь',\n",
       " 'полностью',\n",
       " 'скидки',\n",
       " 'наши',\n",
       " 'адресу',\n",
       " '42',\n",
       " 'москва',\n",
       " 'тк',\n",
       " 'дней',\n",
       " 'передний',\n",
       " 'двигателя',\n",
       " 'то',\n",
       " 'заказ',\n",
       " 'транспортной',\n",
       " '46',\n",
       " 'вес',\n",
       " 'ваш',\n",
       " '35',\n",
       " 'iphone',\n",
       " 'участок',\n",
       " 'ஜ',\n",
       " 'выходных',\n",
       " '13',\n",
       " '2014',\n",
       " '21',\n",
       " 'всё',\n",
       " 'тип',\n",
       " 'почтой',\n",
       " 'коробка',\n",
       " 'сапоги',\n",
       " '1000',\n",
       " 'компании',\n",
       " '70',\n",
       " 'пальто',\n",
       " 'зимние',\n",
       " '2012',\n",
       " 'дверь',\n",
       " 'whatsapp',\n",
       " 'всей',\n",
       " 'подходит',\n",
       " 'магазины',\n",
       " 'безопасности',\n",
       " 'bmw',\n",
       " '2015',\n",
       " 'двери',\n",
       " 'девочки',\n",
       " 'натуральная',\n",
       " 'осень',\n",
       " 'количество',\n",
       " '2013',\n",
       " '22',\n",
       " 'характеристики',\n",
       " 'всех',\n",
       " 'мальчика',\n",
       " 'смотрите',\n",
       " 'toyota',\n",
       " 'может',\n",
       " 'имеется',\n",
       " 'кузов',\n",
       " 'продажа',\n",
       " 'любые',\n",
       " '\",',\n",
       " 'перед',\n",
       " 'внутри',\n",
       " 'viber',\n",
       " 'товаров',\n",
       " 'компания',\n",
       " 'сот',\n",
       " 'детские',\n",
       " 'могу',\n",
       " '45',\n",
       " 'x',\n",
       " '150',\n",
       " 'участке',\n",
       " '2011',\n",
       " 'возможность',\n",
       " 'покупке',\n",
       " 'стекло',\n",
       " 'складе',\n",
       " 'я',\n",
       " 'мех',\n",
       " 'линии',\n",
       " 'нам',\n",
       " '2019',\n",
       " 'пэк',\n",
       " 'больше',\n",
       " 'будет',\n",
       " 'указана',\n",
       " 'сейчас',\n",
       " '55',\n",
       " 'чехол',\n",
       " 'автомобили',\n",
       " 'других',\n",
       " 'колеса',\n",
       " 'c',\n",
       " 'объем',\n",
       " 'этаж',\n",
       " 'чтобы',\n",
       " 'отдам',\n",
       " 'отправляем',\n",
       " '250',\n",
       " '48',\n",
       " 'качества',\n",
       " 'месяцев',\n",
       " 'модели',\n",
       " 'сумка',\n",
       " '2000',\n",
       " 'ford',\n",
       " 'санкт',\n",
       " 'ежедневно',\n",
       " 'находится',\n",
       " 'детский',\n",
       " 'зимний',\n",
       " 'интернет',\n",
       " 'месте',\n",
       " 'продажи',\n",
       " 'услуги',\n",
       " 'renault',\n",
       " '2018',\n",
       " 'телефон',\n",
       " 'производство',\n",
       " 'кроссовки',\n",
       " 'часы',\n",
       " 'оплаты',\n",
       " '2008',\n",
       " 'вашего',\n",
       " 'всего',\n",
       " 'скидка',\n",
       " 'документы',\n",
       " 'л',\n",
       " 'деловые',\n",
       " '38',\n",
       " 'центр',\n",
       " '2016',\n",
       " '2010',\n",
       " 'место',\n",
       " '32',\n",
       " 'европы',\n",
       " 'кредит',\n",
       " 'машина',\n",
       " '2017',\n",
       " 'nissan',\n",
       " '=',\n",
       " 'связи',\n",
       " 'новгород',\n",
       " 'бампера',\n",
       " 'окна',\n",
       " 'комплектация',\n",
       " 'две',\n",
       " 'пару',\n",
       " 'коляска',\n",
       " 'мерседес',\n",
       " 'набор',\n",
       " 'пакет',\n",
       " 'шин',\n",
       " '37',\n",
       " 'компанией',\n",
       " 'брюки',\n",
       " 'идеальное',\n",
       " 'hyundai',\n",
       " 'деталь',\n",
       " 'режим',\n",
       " 'шуба',\n",
       " 'джинсы',\n",
       " '36',\n",
       " 'абсолютно',\n",
       " '400',\n",
       " 'kia',\n",
       " 'срок',\n",
       " 'детская',\n",
       " 'посмотреть',\n",
       " 'кровать',\n",
       " 'audi',\n",
       " 'области',\n",
       " 'акпп',\n",
       " 'запросу',\n",
       " 'новых',\n",
       " 'карту',\n",
       " 'кухня',\n",
       " 'условия',\n",
       " 'еще',\n",
       " '23',\n",
       " 'район',\n",
       " 'описание',\n",
       " '\\\\',\n",
       " 'всем',\n",
       " 'город',\n",
       " '65',\n",
       " 'задняя',\n",
       " 'корпус',\n",
       " 'купить',\n",
       " 'носили',\n",
       " '»,',\n",
       " 'резина',\n",
       " 'дополнительные',\n",
       " 'площадь',\n",
       " 'правая',\n",
       " '...',\n",
       " 'замена',\n",
       " 'ткань',\n",
       " 'вещи',\n",
       " 'документов',\n",
       " 'звонить',\n",
       " 'быстро',\n",
       " 'доме',\n",
       " 'уже',\n",
       " 'японии',\n",
       " 'фирмы',\n",
       " 'шкаф',\n",
       " '\".',\n",
       " 'продаже',\n",
       " 'где',\n",
       " 'класс',\n",
       " '2006',\n",
       " 'п',\n",
       " 'замок',\n",
       " 'черный',\n",
       " 'левая',\n",
       " 'либо',\n",
       " 'квартиру',\n",
       " 'ещё',\n",
       " 'каждый',\n",
       " '2007',\n",
       " 'является',\n",
       " 'его',\n",
       " 'ниже',\n",
       " '90',\n",
       " 'рено',\n",
       " 'газ',\n",
       " 'диск',\n",
       " 'мебель',\n",
       " 'пуховик',\n",
       " 'производитель',\n",
       " '\"/',\n",
       " 'стол',\n",
       " 'проверку',\n",
       " 'несколько',\n",
       " 'v',\n",
       " 'пн',\n",
       " 'доставку',\n",
       " 'любое',\n",
       " 'имеет',\n",
       " 'легко',\n",
       " 'диван',\n",
       " 'квартире',\n",
       " 'нового',\n",
       " '2005',\n",
       " 'фара',\n",
       " 'управления',\n",
       " 'мощность',\n",
       " 'левый',\n",
       " 'мало',\n",
       " 'пр',\n",
       " 'график',\n",
       " 'наших',\n",
       " 'юбка',\n",
       " 'нашего',\n",
       " 'торга',\n",
       " '09',\n",
       " 'производителя',\n",
       " 'позволяет',\n",
       " 'центре',\n",
       " 'белый',\n",
       " 'передняя',\n",
       " 'сидений',\n",
       " 'диаметр',\n",
       " 'мин',\n",
       " 'ссср',\n",
       " 'usb',\n",
       " 'зима',\n",
       " 'часов',\n",
       " 'хлопок',\n",
       " 'm',\n",
       " 'samsung',\n",
       " '·',\n",
       " 'сборе',\n",
       " 'хорошо',\n",
       " 'удобная',\n",
       " 'ни',\n",
       " 'этаже',\n",
       " 'цену',\n",
       " 'стельке',\n",
       " 'менеджера',\n",
       " 'продаётся',\n",
       " 'отлично',\n",
       " '120',\n",
       " 'возможно',\n",
       " 'цене',\n",
       " 'удобные',\n",
       " 'раза',\n",
       " '2009',\n",
       " 'квартиры',\n",
       " 'выпуска',\n",
       " 'девочку',\n",
       " 'оригинальные',\n",
       " 'минут',\n",
       " 'сезон',\n",
       " 'профиле',\n",
       " 'н',\n",
       " '?',\n",
       " 'правый',\n",
       " 'багажника',\n",
       " 'городу',\n",
       " 'вся',\n",
       " 'пробегом',\n",
       " 'доступности',\n",
       " 'зимняя',\n",
       " 'салон',\n",
       " 'почти',\n",
       " '52',\n",
       " 'датчик',\n",
       " 'прямо',\n",
       " 'гараж',\n",
       " 'видео',\n",
       " 'форд',\n",
       " 'a',\n",
       " 'пробега',\n",
       " 'двух',\n",
       " 'стоит',\n",
       " 'монтаж',\n",
       " 'размера',\n",
       " 'огромный',\n",
       " '350',\n",
       " 'ваз',\n",
       " '):',\n",
       " 'регулировка',\n",
       " 'просто',\n",
       " '110',\n",
       " 'рабочем',\n",
       " '-/',\n",
       " '26',\n",
       " 'он',\n",
       " 'договор',\n",
       " 'себя',\n",
       " 'устройство',\n",
       " 'фары',\n",
       " 'нижний',\n",
       " 'заднего',\n",
       " '..',\n",
       " 'отличный',\n",
       " 'усилитель',\n",
       " 'l',\n",
       " 'in',\n",
       " 'даже',\n",
       " 'срочно',\n",
       " '✔️',\n",
       " 'районе',\n",
       " 'имеются',\n",
       " '39',\n",
       " 'крыло',\n",
       " 'школа',\n",
       " 'pro',\n",
       " 'клиентам',\n",
       " 'регион',\n",
       " 'марка',\n",
       " 'объявления',\n",
       " 'системы',\n",
       " 'поэтому',\n",
       " 'их',\n",
       " 'шиномонтаж',\n",
       " 'сша',\n",
       " 'другой',\n",
       " 'около',\n",
       " 'предоплаты',\n",
       " 'автозапчастей',\n",
       " 'выкуп',\n",
       " 'работа',\n",
       " 'моделей',\n",
       " 'комнаты',\n",
       " 'три',\n",
       " 'контроль',\n",
       " 'часть',\n",
       " 'ценам',\n",
       " 'склад',\n",
       " '1500',\n",
       " 'данный',\n",
       " '28',\n",
       " 'телевизор',\n",
       " 'volkswagen',\n",
       " 'нужно',\n",
       " 'уместен',\n",
       " 'гб',\n",
       " 'постоянным',\n",
       " 'потолки',\n",
       " 'расчет',\n",
       " 'фотографии',\n",
       " 'петербург',\n",
       " 'mitsubishi',\n",
       " 'дефектов',\n",
       " 'удобный',\n",
       " 'двс',\n",
       " 'состав',\n",
       " 'код',\n",
       " 'практически',\n",
       " 'пробег',\n",
       " 'большая',\n",
       " 'рубашка',\n",
       " 'женские',\n",
       " 'class',\n",
       " 'подушка',\n",
       " '800',\n",
       " '₽/',\n",
       " 'весна',\n",
       " 'сдэк',\n",
       " 'r',\n",
       " 'немного',\n",
       " 'вместе',\n",
       " 'воды',\n",
       " '———————————————————————————/',\n",
       " '☎',\n",
       " 'память',\n",
       " 'наушники',\n",
       " 'деталей',\n",
       " 'места',\n",
       " 'стекла',\n",
       " 'стороны',\n",
       " 'передние',\n",
       " 'краснодар',\n",
       " 'чем',\n",
       " 'детей',\n",
       " 'руля',\n",
       " 'находятся',\n",
       " 'номера',\n",
       " 'мес',\n",
       " 'вещей',\n",
       " '600',\n",
       " 'любую',\n",
       " 'вопросам',\n",
       " 'безналичный',\n",
       " 'вода',\n",
       " 'собственник',\n",
       " '➡',\n",
       " 'кожи',\n",
       " 'который',\n",
       " 'том',\n",
       " 'доставки',\n",
       " 'зеркало',\n",
       " 'сб',\n",
       " 'оборудование',\n",
       " '31',\n",
       " 'гарантии',\n",
       " 'обогрев',\n",
       " 'др',\n",
       " 'подойдет',\n",
       " '75',\n",
       " 'сад',\n",
       " 'ниссан',\n",
       " 'защита',\n",
       " 'ваши',\n",
       " 'счет',\n",
       " 'фирма',\n",
       " '225',\n",
       " 'памяти',\n",
       " 'e',\n",
       " 'помещение',\n",
       " 'синий',\n",
       " 'вс',\n",
       " 'покупали',\n",
       " 'месяца',\n",
       " 'заказа',\n",
       " 'сообщения',\n",
       " 'был',\n",
       " 'комната',\n",
       " 'сайт',\n",
       " 'жк',\n",
       " '|',\n",
       " 'информацию',\n",
       " 'мкад',\n",
       " 'небольшой',\n",
       " 'наличный',\n",
       " 'теплый',\n",
       " 'ручка',\n",
       " 'рады',\n",
       " 'магазинов',\n",
       " 'отдельно',\n",
       " 'benz',\n",
       " 'приобрести',\n",
       " 'покупала',\n",
       " 'дисков',\n",
       " 'шаговой',\n",
       " 'любом',\n",
       " 'тц',\n",
       " 'опыт',\n",
       " 'вариант',\n",
       " 'использовать',\n",
       " 'предоставляем',\n",
       " 'хороший',\n",
       " 'внимание',\n",
       " 'радиатора',\n",
       " 'птс',\n",
       " 'плитка',\n",
       " 'именно',\n",
       " 'свой',\n",
       " 'молнии',\n",
       " 'натуральный',\n",
       " 'дня',\n",
       " 'пт',\n",
       " 'упаковке',\n",
       " 'помощь',\n",
       " '27',\n",
       " 'сапожки',\n",
       " '.)',\n",
       " 'покупки',\n",
       " 'новой',\n",
       " 'месяц',\n",
       " 'обувь',\n",
       " 'накладка',\n",
       " 'этого',\n",
       " 'opel',\n",
       " 'остальные',\n",
       " 'шапка',\n",
       " 'оптом',\n",
       " '3d',\n",
       " 'панель',\n",
       " 'потолок',\n",
       " 'арт',\n",
       " 'маркировка',\n",
       " 'дону',\n",
       " '●',\n",
       " 'ребенка',\n",
       " 'часа',\n",
       " '3000',\n",
       " 'камера',\n",
       " 'питания',\n",
       " 'контакты',\n",
       " 'м2',\n",
       " 'которые',\n",
       " 'киа',\n",
       " 'босоножки',\n",
       " 'этот',\n",
       " 'любых',\n",
       " 'просьба',\n",
       " 'машины',\n",
       " 'оборудования',\n",
       " 'h',\n",
       " 'метров',\n",
       " 'осуществляется',\n",
       " '92',\n",
       " 'руль',\n",
       " 'кпп',\n",
       " 'бмв',\n",
       " 'можем',\n",
       " 'дешевле',\n",
       " 'рукава',\n",
       " 'форма',\n",
       " 'идеально',\n",
       " '700',\n",
       " 'mazda',\n",
       " '*/',\n",
       " 'крышка',\n",
       " '54',\n",
       " 'тойота',\n",
       " 'ростов',\n",
       " 'производства',\n",
       " '140',\n",
       " 'подойдёт',\n",
       " 'trade',\n",
       " 'поддержка',\n",
       " 'штаны',\n",
       " 'работ',\n",
       " 'радиатор',\n",
       " 'получите',\n",
       " 'ремонта',\n",
       " 'комплекс',\n",
       " 'насос',\n",
       " 'товары',\n",
       " 'теплая',\n",
       " 'шоссе',\n",
       " 'техники',\n",
       " 'чемодан',\n",
       " 'самые',\n",
       " 'переплат',\n",
       " 'менеджеру',\n",
       " 'части',\n",
       " 'вид',\n",
       " 'd',\n",
       " 'удобно',\n",
       " 'гарантией',\n",
       " 'ауди',\n",
       " '56',\n",
       " 'купли',\n",
       " 'грузовой',\n",
       " 'час',\n",
       " 'этом',\n",
       " 'темно',\n",
       " 'примерно',\n",
       " 'процессор',\n",
       " 'парк',\n",
       " 'оптовым',\n",
       " 'смс',\n",
       " '₽',\n",
       " '2003',\n",
       " 'пол',\n",
       " 'ключ',\n",
       " '500р',\n",
       " 'энергия',\n",
       " 'гарантию',\n",
       " 'данные',\n",
       " '%/',\n",
       " 'игры',\n",
       " 'предлагает',\n",
       " 'нашей',\n",
       " 'преимущества',\n",
       " 'офис',\n",
       " 'виде',\n",
       " 'объявление',\n",
       " 'хранения',\n",
       " 'вт',\n",
       " 'капюшон',\n",
       " 'проверки',\n",
       " 'одной',\n",
       " 'fi',\n",
       " 'предоставим',\n",
       " 'тёплый',\n",
       " 'сторона',\n",
       " 'разделе',\n",
       " 'италия',\n",
       " 'отделка',\n",
       " '41',\n",
       " 'plus',\n",
       " 'установку',\n",
       " 'рестайлинг',\n",
       " '&',\n",
       " 'находимся',\n",
       " 'телефона',\n",
       " 'рабочий',\n",
       " 'мебели',\n",
       " 'oem',\n",
       " 'одного',\n",
       " 'подушки',\n",
       " 'подкладка',\n",
       " 'фонарь',\n",
       " 'удобное',\n",
       " 'найти',\n",
       " 'зеркал',\n",
       " 'комплекта',\n",
       " 'фольксваген',\n",
       " 'снг',\n",
       " '33',\n",
       " 'воронеж',\n",
       " 'склада',\n",
       " 'натяжные',\n",
       " 'ремень',\n",
       " 'низкие',\n",
       " '#',\n",
       " 'внутренний',\n",
       " 'холодильник',\n",
       " 'профиль',\n",
       " 'задние',\n",
       " 'конверт',\n",
       " 'эксплуатации',\n",
       " '.)/',\n",
       " 'запчасть',\n",
       " 'стр',\n",
       " '29',\n",
       " '2004',\n",
       " 'наличными',\n",
       " 'кожаные',\n",
       " 'газель',\n",
       " '98',\n",
       " 'красивый',\n",
       " 'экран',\n",
       " 'volvo',\n",
       " 'цветов',\n",
       " 'благодаря',\n",
       " 'привод',\n",
       " 'водителя',\n",
       " 'мотор',\n",
       " 'вашей',\n",
       " 'широкий',\n",
       " 'объявлении',\n",
       " 'видно',\n",
       " '86',\n",
       " 'да',\n",
       " '×',\n",
       " 'одна',\n",
       " 'одежды',\n",
       " 'w',\n",
       " 'отправлю',\n",
       " '64',\n",
       " 'максимальная',\n",
       " 'кабель',\n",
       " 'продаются',\n",
       " 'подробности',\n",
       " 'предлагаем',\n",
       " 'изделия',\n",
       " 'работу',\n",
       " '34',\n",
       " 'шерсть',\n",
       " 'центральный',\n",
       " 'общая',\n",
       " 'натуральной',\n",
       " 'англии',\n",
       " 'готовы',\n",
       " 'осуществляем',\n",
       " ');/',\n",
       " 'intel',\n",
       " 'данная',\n",
       " 'получении',\n",
       " 'использовался',\n",
       " 'капот',\n",
       " 'волгоград',\n",
       " 'акция',\n",
       " 'дизайн',\n",
       " 'пределах',\n",
       " 'отличная',\n",
       " '128',\n",
       " 'багажник',\n",
       " 'владимир',\n",
       " 'белгород',\n",
       " 'sony',\n",
       " 'пластик',\n",
       " 'оформление',\n",
       " 'россия',\n",
       " 'продаем',\n",
       " 'территории',\n",
       " 'решетка',\n",
       " 'chevrolet',\n",
       " 'сделать',\n",
       " 'школы',\n",
       " 'спб',\n",
       " 'вашему',\n",
       " 'клиентов',\n",
       " 'разных',\n",
       " 'рассрочка',\n",
       " 'кит',\n",
       " 'материалов',\n",
       " 'деньги',\n",
       " 'парка',\n",
       " 'класса',\n",
       " 'тёплая',\n",
       " '2002',\n",
       " 'брянск',\n",
       " 'указанный',\n",
       " 'скидку',\n",
       " 'сразу',\n",
       " 'кузова',\n",
       " 'подарки',\n",
       " 'кондиционера',\n",
       " 'марки',\n",
       " 'g',\n",
       " 'bluetooth',\n",
       " 'land',\n",
       " '_',\n",
       " 'выезд',\n",
       " '74',\n",
       " 'мир',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list = sorted_tokens[:10000]\n",
    "tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, которая переводит текст в вектор из чисел. То есть каждому токену из списка токенов сопоставляется количество раз, которое он встретился в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4awkhecbR9om"
   },
   "outputs": [],
   "source": [
    "def text_to_bow(text: str, tokens_list: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указано количество его употреблений в предложении\n",
    "    input: строка, список токенов\n",
    "    output: вектор той же размерности, что и список токенов\n",
    "    \"\"\"\n",
    "    string = text.split()\n",
    "    # создаем вектор той же размерности, что и список токенов\n",
    "    vec = np.zeros(len(tokens_list))\n",
    "    for word in string:\n",
    "        if word in tokens_list:\n",
    "            # учитываем появление слова в векторе \n",
    "            vec[tokens_list.index(word)] += 1\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\", tokens_list)\n",
    "\n",
    "assert np.allclose(example_text.mean(), 0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и каждому тексту из `'description'` сопоставляет вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "def descr_to_bow(items: np.array, tokens_list: list) -> np.array:\n",
    "    \"\"\" Для каждого описания товара возвращает вектор его bow \"\"\"\n",
    "    return np.array([text_to_bow(description, tokens_list) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "8a30c3af-3517-42bd-a5f3-36206b4b264a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:20<00:00, 261.80it/s]\n",
      "100%|██████████| 9000/9000 [00:33<00:00, 265.05it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = descr_to_bow(X_train, tokens_list)\n",
    "X_test_bow = descr_to_bow(X_test, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_bow.shape == (21000, 10000), X_test_bow.shape == (9000, 10000)\n",
    "assert 0.005 < X_train_bow.mean() < 0.006\n",
    "assert 0.005 < X_test_bow.mean() < 0.006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVM (0.5 балла)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве — какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVM с линейным ядром (`sklearn.svm.LinearSVC` или `sklearn.svm.SVC(kernel='linear')`) с базовыми параметрами. При необходимости можете увеличить максимальное число итераций. В качестве `random_state` возьмите 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Подсказка: для того, чтобы было проще обучать, можно использовать [разреженные матрицы](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0), многие модели из `sklearn` умеют с ними работать. Соответствующий модуль из `scipy`: [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html). Нетрудно заметить, что в полученных BOW-матрицах очень много нулей. Если хранить в памяти только ненулевые элементы, можно сильно оптимизировать вычисления. Можете в этом убедиться:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train array in memory (raw): 1680.000 Mb\n",
      "Train array in memory (compressed): 8.606 Mb\n"
     ]
    }
   ],
   "source": [
    "print('Train array in memory (raw): {:.3f} Mb'.format(X_train_bow.nbytes * 1e-6))\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_bow_csr = csr_matrix(X_train_bow)\n",
    "print('Train array in memory (compressed): {:.3f} Mb'.format(\n",
    "    (X_train_bow_csr.data.nbytes + X_train_bow_csr.indptr.nbytes + X_train_bow_csr.indices.nbytes) * 1e-6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow_csr = csr_matrix(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>1. Логистическая регрессия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=13, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=13)\n",
    "lr.fit(X_train_bow_csr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "612a5f0d-76bd-44f4-eeeb-63b517443797"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test_bow_csr)\n",
    "\n",
    "assert accuracy_score(y_test, y_pred) > 0.695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7011111111111111\n"
     ]
    }
   ],
   "source": [
    "accuracy_1_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_1_logreg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=13, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM_linear_classifier = LinearSVC(random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_bow_csr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [],
   "source": [
    "y_pred = SVM_linear_classifier.predict(X_test_bow_csr)\n",
    "\n",
    "assert accuracy_score(y_test, y_pred) > 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.684\n"
     ]
    }
   ],
   "source": [
    "accuracy_1_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_1_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "<br>1. Прибавьте к соответствующим BOW-векторам BOW-вектора для `'title'` товара с некоторым весом. Изменится ли качество? Как вы можете это объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы создать BOW-вектора для `'title'` необходимо определить функцию `title_to_bow` по аналогии с `descr_to_bow`. Сделаем это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_bow(items: np.array, tokens_list: list) -> np.array:\n",
    "    \"\"\" Для каждого заглавия товара возвращает вектор его bow \"\"\"\n",
    "    return np.array([text_to_bow(title, tokens_list) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:09<00:00, 2256.07it/s]\n",
      "100%|██████████| 9000/9000 [00:08<00:00, 1015.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_title_bow = title_to_bow(X_train, tokens_list)\n",
    "X_test_title_bow = title_to_bow(X_test, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прибавим к соответствующим BOW-векторам BOW-вектора для 'title' товара с весом большем единицы. Например, с весом 2 (будем считать заголовок в 2 раза релевантнее описания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_full = 2 * X_train_title_bow + X_train_bow\n",
    "X_test_bow_full = 2 * X_test_title_bow + X_test_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем также использовать разреженные матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_csr = csr_matrix(X_train_bow_full)\n",
    "X_test_bow_csr = csr_matrix(X_test_bow_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, увеличилось ли качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Логистическая регрессия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13, max_iter=500)\n",
    "lr.fit(X_train_bow_csr, y_train)\n",
    "y_pred = lr.predict(X_test_bow_csr)\n",
    "accuracy_2_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_2_logreg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение: 0.13%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение: {round((accuracy_2_logreg - accuracy_1_logreg) / accuracy_1_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7605555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_bow_csr, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_bow_csr)\n",
    "accuracy_2_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_2_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение: 0.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение: {round((accuracy_2_svm - accuracy_1_svm) / accuracy_1_svm, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что качество у обоих методов выросло. Обычно `'title'` — это обобщение `'description'`. Нередко в `'title'` пишется как раз товар, который мы пытаем категоризовать, а в `'description'` — какие-то его характеристики. Следовательно, наша обучающая выборка после прибавления к BOW-векторам для `'description'` BOW-вектора для 'title' становится более релевантной, информативной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "<br>2. Нормализуйте данные с помощью `MinMaxScaler` или `MaxAbsScaler` перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J"
   },
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_bow_full)\n",
    "X_test_scaled = scaler.transform(X_test_bow_full)\n",
    "X_train_scaled_csr = csr_matrix(X_train_scaled)\n",
    "X_test_scaled_csr = csr_matrix(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, увеличилось ли качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7477777777777778\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13, max_iter=500)\n",
    "lr.fit(X_train_scaled_csr, y_train)\n",
    "y_pred = lr.predict(X_test_scaled_csr)\n",
    "accuracy_3_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_3_logreg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с моделью без модификаций: 0.07%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с моделью без модификаций: {round((accuracy_3_logreg - accuracy_1_logreg) / accuracy_1_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логистической регресси при нормализации данных качество упало, если сравнивать с первым пунктом, но зато теперь решение сходиться к локальному минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное ухудшение при сравнении с моделью с первой модификацией: -0.05%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное ухудшение при сравнении с моделью с первой модификацией: {round((accuracy_3_logreg - accuracy_2_logreg) / accuracy_2_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно ухудшение связано с тем, что нормальные признаки имели достаточно большой размах, а шумовые признаки — маленький. При масштабировании же мы привели все признаки к единому масштабу и от этого ухудшилось качество в логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "б) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7767777777777778\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_scaled_csr, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_scaled_csr)\n",
    "accuracy_3_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_3_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с моделью без модификаций: 0.14%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с моделью без модификаций: {round((accuracy_3_svm - accuracy_1_svm) / accuracy_1_svm, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с моделью с первой модификацией: 0.02%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с моделью с первой модификацией: {round((accuracy_3_svm - accuracy_2_svm) / accuracy_2_svm, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для SVM при нормализации данных качество улучшилось и теперь решение сходиться к минимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему в данном случае использовать `StandardScaler` — не очень хорошая идея?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, из-за деления на ноль (когда дисперсия равна нулю): в таком случае будет вылезать nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### Иная предобработка (1 балл)\n",
    "\n",
    "**На выбор**:\n",
    "\n",
    "- **либо** обучите модели, используя для предобработки токенизатор и лемматизатор `pymystem3.Mystem`.\n",
    "- **либо** добавьте к предобработке стэмминг.\n",
    "\n",
    "Сравните полученное сейчас качество с полученным ранее и сделайте вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "source": [
    "Выберем в качестве дополнительной предоработки лемматизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем лемматизировать поочередно все объявления (`string`) в обучающей выборке и добавлять в словарь `tokens_cnt` какждое новое вхождение конкретного слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:44<00:00, 477.03it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_cnt_lemmatized = {}\n",
    "for title, description in tqdm(X_train):\n",
    "    string = title + ' ' + description\n",
    "    for word in mystem_analyzer.lemmatize(string):\n",
    "         tokens_cnt_lemmatized[word] = tokens_cnt_lemmatized.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем наш словарь и оставим только топ-10000 слов, как делали это и раньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens_lemmatized = sorted(tokens_cnt_lemmatized, key=lambda x: tokens_cnt_lemmatized.get(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'и',\n",
       " 'на',\n",
       " 'с',\n",
       " 'по',\n",
       " 'для',\n",
       " 'состояние',\n",
       " 'новый',\n",
       " 'не',\n",
       " 'продавать',\n",
       " 'весь',\n",
       " 'размер',\n",
       " 'до',\n",
       " 'от',\n",
       " 'быть',\n",
       " 'цена',\n",
       " 'у',\n",
       " 'доставка',\n",
       " 'за',\n",
       " 'без',\n",
       " 'вы',\n",
       " 'хороший',\n",
       " 'отличный',\n",
       " 'из',\n",
       " 'год',\n",
       " 'наш',\n",
       " 'запчасть',\n",
       " 'наличие',\n",
       " 'см',\n",
       " 'магазин',\n",
       " 'к',\n",
       " 'мы',\n",
       " 'или',\n",
       " 'любой',\n",
       " '═',\n",
       " 'автомобиль',\n",
       " 'б',\n",
       " 'квартира',\n",
       " 'звонить',\n",
       " 'руб',\n",
       " 'при',\n",
       " 'комплект',\n",
       " 'очень',\n",
       " 'компания',\n",
       " 'гарантия',\n",
       " 'работа',\n",
       " 'дом',\n",
       " 'р',\n",
       " 'фото',\n",
       " 'товар',\n",
       " 'можно',\n",
       " 'м',\n",
       " 'мочь',\n",
       " 'работать',\n",
       " 'цвет',\n",
       " 'ваш',\n",
       " 'другой',\n",
       " 'один',\n",
       " 'платье',\n",
       " 'телефон',\n",
       " 'задний',\n",
       " 'регион',\n",
       " 'торг',\n",
       " 'возможный',\n",
       " 'под',\n",
       " 'день',\n",
       " 'как',\n",
       " 'номер',\n",
       " 'большой',\n",
       " 'все',\n",
       " 'россия',\n",
       " 'транспортный',\n",
       " 'а',\n",
       " 'детский',\n",
       " 'так',\n",
       " 'оригинал',\n",
       " 'г',\n",
       " 'мм',\n",
       " 'подходить',\n",
       " '↓',\n",
       " 'передний',\n",
       " 'город',\n",
       " 'система',\n",
       " 'шина',\n",
       " 'раз',\n",
       " 'двигатель',\n",
       " 'зимний',\n",
       " 'скидка',\n",
       " 'продажа',\n",
       " 'ремонт',\n",
       " 'два',\n",
       " 'модель',\n",
       " 'москва',\n",
       " 'диск',\n",
       " '✔',\n",
       " 'полный',\n",
       " 'деталь',\n",
       " 'куртка',\n",
       " 'только',\n",
       " 'много',\n",
       " 'удобный',\n",
       " 'адрес',\n",
       " 'оригинальный',\n",
       " 'оплата',\n",
       " 'что',\n",
       " 'шт',\n",
       " '▬',\n",
       " 'качество',\n",
       " 'теплый',\n",
       " 'длина',\n",
       " 'время',\n",
       " 'место',\n",
       " 'писать',\n",
       " 'участок',\n",
       " 'отправка',\n",
       " 'также',\n",
       " 'дверь',\n",
       " 'идеальный',\n",
       " 'материал',\n",
       " 'рубль',\n",
       " 'кожа',\n",
       " 'ул',\n",
       " 'натуральный',\n",
       " 'ассортимент',\n",
       " 'установка',\n",
       " 'заказ',\n",
       " 'д',\n",
       " 'продаваться',\n",
       " 'артикул',\n",
       " 'же',\n",
       " 'подарок',\n",
       " 'высота',\n",
       " 'сайт',\n",
       " 'свой',\n",
       " 'бампер',\n",
       " 'блок',\n",
       " 'нет',\n",
       " 'отправлять',\n",
       " 'со',\n",
       " 'уточнять',\n",
       " 'машина',\n",
       " 'склад',\n",
       " 'девочка',\n",
       " 'этот',\n",
       " 'более',\n",
       " 'покупка',\n",
       " 'карта',\n",
       " 'это',\n",
       " 'вопрос',\n",
       " 'ребенок',\n",
       " 'указывать',\n",
       " 'купить',\n",
       " 'колесо',\n",
       " 'самый',\n",
       " 'находиться',\n",
       " 'авто',\n",
       " 'рабочий',\n",
       " '✅',\n",
       " 'центр',\n",
       " 'черный',\n",
       " 'обмен',\n",
       " 'коробка',\n",
       " 'но',\n",
       " 'х',\n",
       " 'который',\n",
       " 'каждый',\n",
       " 'стекло',\n",
       " 'этаж',\n",
       " 'т',\n",
       " 'почта',\n",
       " 'эта',\n",
       " 'выбор',\n",
       " 'правый',\n",
       " 'месяц',\n",
       " 'комбинезон',\n",
       " 'бесплатный',\n",
       " 'если',\n",
       " 'левый',\n",
       " 'вид',\n",
       " 'рф',\n",
       " 'км',\n",
       " 'документ',\n",
       " 'выходной',\n",
       " 'дополнительный',\n",
       " 'пробег',\n",
       " 'то',\n",
       " 'костюм',\n",
       " 'производитель',\n",
       " 'стоимость',\n",
       " 'после',\n",
       " 'час',\n",
       " 'кг',\n",
       " 'я',\n",
       " 'чехол',\n",
       " 'район',\n",
       " 'данный',\n",
       " 'иметься',\n",
       " 'кв',\n",
       " 'он',\n",
       " 'менеджер',\n",
       " 'белый',\n",
       " 'рост',\n",
       " 'носить',\n",
       " 'кузов',\n",
       " 'клиент',\n",
       " 'самовывоз',\n",
       " 'авито',\n",
       " 'потолок',\n",
       " 'бесплатно',\n",
       " 'объявление',\n",
       " '№',\n",
       " 'комната',\n",
       " 'ч',\n",
       " 'тип',\n",
       " 'красивый',\n",
       " 'бу',\n",
       " 'сидение',\n",
       " 'всегда',\n",
       " 'фара',\n",
       " 'мех',\n",
       " 'туфля',\n",
       " 'высокий',\n",
       " 'вещь',\n",
       " 'коляска',\n",
       " 'покупать',\n",
       " 'безопасность',\n",
       " 'предлагать',\n",
       " 'пара',\n",
       " 'ширина',\n",
       " 'еще',\n",
       " 'производство',\n",
       " 's',\n",
       " 'вода',\n",
       " 'метро',\n",
       " 'услуга',\n",
       " 'смотреть',\n",
       " 'объем',\n",
       " 'ботинок',\n",
       " 'сделать',\n",
       " 'возможность',\n",
       " 'мальчик',\n",
       " 'проверка',\n",
       " 'условие',\n",
       " 'о',\n",
       " 'вес',\n",
       " 'качественный',\n",
       " 'во',\n",
       " 'сторона',\n",
       " 'мебель',\n",
       " 'устанавливать',\n",
       " 'часть',\n",
       " 'через',\n",
       " 'количество',\n",
       " 'срок',\n",
       " 'женский',\n",
       " 'кухня',\n",
       " 'полностью',\n",
       " 'зима',\n",
       " 'фирма',\n",
       " 'mercedes',\n",
       " 'отдавать',\n",
       " 'рядом',\n",
       " 'салон',\n",
       " '₽',\n",
       " 'характеристика',\n",
       " 'получать',\n",
       " 'корпус',\n",
       " 'тк',\n",
       " 'окно',\n",
       " 'зеркало',\n",
       " 'осень',\n",
       " 'сумка',\n",
       " 'сапог',\n",
       " 'iphone',\n",
       " 'профиль',\n",
       " 'ஜ',\n",
       " 'резина',\n",
       " 'техника',\n",
       " 'класс',\n",
       " 'одевать',\n",
       " 'отвечать',\n",
       " 'площадь',\n",
       " 'пакет',\n",
       " 'мягкий',\n",
       " 'пальто',\n",
       " 'оборудование',\n",
       " 'линия',\n",
       " 'предоставлять',\n",
       " 'режим',\n",
       " '●',\n",
       " 'покупатель',\n",
       " 'whatsapp',\n",
       " 'интернет',\n",
       " 'ткань',\n",
       " 'багажник',\n",
       " 'они',\n",
       " 'bmw',\n",
       " 'радиатор',\n",
       " 'первый',\n",
       " 'иметь',\n",
       " 'стелька',\n",
       " 'подушка',\n",
       " 'школа',\n",
       " 'комплектация',\n",
       " 'низкий',\n",
       " 'вариант',\n",
       " 'тот',\n",
       " 'память',\n",
       " 'набор',\n",
       " 'позволять',\n",
       " 'упаковка',\n",
       " 'руль',\n",
       " 'шуба',\n",
       " 'шкаф',\n",
       " 'штука',\n",
       " 'рукав',\n",
       " 'такой',\n",
       " 'toyota',\n",
       " 'связь',\n",
       " 'чистый',\n",
       " 'внутренний',\n",
       " 'минута',\n",
       " 'управление',\n",
       " 'марка',\n",
       " 'сотня',\n",
       " 'небольшой',\n",
       " 'кровать',\n",
       " 'кредит',\n",
       " 'перед',\n",
       " 'выполнять',\n",
       " 'синий',\n",
       " 'замена',\n",
       " 'форма',\n",
       " 'внутри',\n",
       " 'игра',\n",
       " 'viber',\n",
       " 'нижний',\n",
       " 'себя',\n",
       " 'использовать',\n",
       " 'готовый',\n",
       " 'запрос',\n",
       " 'стол',\n",
       " 'европа',\n",
       " 'использоваться',\n",
       " 'замок',\n",
       " 'парк',\n",
       " 'x',\n",
       " 'помещение',\n",
       " 'разный',\n",
       " 'пэк',\n",
       " 'область',\n",
       " 'ручка',\n",
       " 'помощь',\n",
       " 'постоянный',\n",
       " 'находить',\n",
       " 'являться',\n",
       " 'пол',\n",
       " 'договор',\n",
       " 'информация',\n",
       " 'деловой',\n",
       " 'сообщение',\n",
       " 'метр',\n",
       " 'официальный',\n",
       " 'расчет',\n",
       " 'сейчас',\n",
       " 'кожаный',\n",
       " 'прямой',\n",
       " 'летний',\n",
       " 'устройство',\n",
       " 'предоплата',\n",
       " 'новгород',\n",
       " 'сад',\n",
       " 'c',\n",
       " 'камера',\n",
       " 'красный',\n",
       " 'натяжной',\n",
       " 'чтобы',\n",
       " 'сезон',\n",
       " 'петербург',\n",
       " '❀',\n",
       " 'ремень',\n",
       " 'крышка',\n",
       " 'юбка',\n",
       " 'помогать',\n",
       " 'приобретать',\n",
       " 'мой',\n",
       " 'ford',\n",
       " 'санкт',\n",
       " 'ежедневно',\n",
       " 'весна',\n",
       " 'ключ',\n",
       " 'брюки',\n",
       " 'дефект',\n",
       " 'хорошо',\n",
       " 'наличный',\n",
       " 'легкий',\n",
       " 'стоять',\n",
       " 'технический',\n",
       " 'посмотреть',\n",
       " 'renault',\n",
       " 'защита',\n",
       " 'центральный',\n",
       " 'отделка',\n",
       " 'регулировка',\n",
       " 'она',\n",
       " 'кроссовок',\n",
       " 'диван',\n",
       " 'огромный',\n",
       " 'звонок',\n",
       " 'располагать',\n",
       " 'несколько',\n",
       " 'счет',\n",
       " 'маленький',\n",
       " 'мощность',\n",
       " 'книга',\n",
       " 'улица',\n",
       " 'три',\n",
       " 'гараж',\n",
       " 'л',\n",
       " 'фотография',\n",
       " 'проходить',\n",
       " 'капюшон',\n",
       " 'экран',\n",
       " 'датчик',\n",
       " 'мерседес',\n",
       " 'акпп',\n",
       " 'nissan',\n",
       " 'диаметр',\n",
       " 'джинсы',\n",
       " 'описание',\n",
       " 'аренда',\n",
       " 'оставаться',\n",
       " 'максимальный',\n",
       " 'япония',\n",
       " 'комплекс',\n",
       " 'мало',\n",
       " 'газ',\n",
       " 'продавец',\n",
       " 'офис',\n",
       " 'предложение',\n",
       " 'нужный',\n",
       " 'жилой',\n",
       " 'давать',\n",
       " 'представлять',\n",
       " 'банк',\n",
       " 'акция',\n",
       " 'молния',\n",
       " 'hyundai',\n",
       " 'выезд',\n",
       " 'панель',\n",
       " 'автозапчасть',\n",
       " 'общий',\n",
       " 'крыло',\n",
       " 'изделие',\n",
       " 'группа',\n",
       " 'абсолютно',\n",
       " 'одежда',\n",
       " 'двор',\n",
       " 'kia',\n",
       " 'необходимый',\n",
       " 'audi',\n",
       " 'разборка',\n",
       " 'безналичный',\n",
       " 'обувь',\n",
       " 'мужской',\n",
       " 'площадка',\n",
       " 'использование',\n",
       " 'боковой',\n",
       " 'выгодный',\n",
       " 'быстро',\n",
       " 'бренд',\n",
       " 'монтаж',\n",
       " 'свет',\n",
       " 'ноутбук',\n",
       " 'хлопок',\n",
       " 'краснодар',\n",
       " 'собственник',\n",
       " 'территория',\n",
       " 'грузовой',\n",
       " 'привозить',\n",
       " 'сервис',\n",
       " 'график',\n",
       " 'кондиционер',\n",
       " 'рука',\n",
       " 'оптовый',\n",
       " 'мир',\n",
       " 'часы',\n",
       " 'сам',\n",
       " '☎',\n",
       " 'хранение',\n",
       " 'капот',\n",
       " 'усилитель',\n",
       " 'крепление',\n",
       " 'питание',\n",
       " 'доступность',\n",
       " 'забирать',\n",
       " 'уже',\n",
       " 'утеплять',\n",
       " 'пуховик',\n",
       " 'легко',\n",
       " 'получение',\n",
       " 'контроль',\n",
       " 'где',\n",
       " 'накладка',\n",
       " 'спортивный',\n",
       " 'п',\n",
       " 'либо',\n",
       " 'телевизор',\n",
       " 'аккумулятор',\n",
       " 'рынок',\n",
       " 'крыша',\n",
       " 'опыт',\n",
       " 'шапка',\n",
       " 'индивидуальный',\n",
       " 'привод',\n",
       " 'рено',\n",
       " 'современный',\n",
       " 'сбор',\n",
       " 'малыш',\n",
       " 'v',\n",
       " 'компьютер',\n",
       " 'пн',\n",
       " 'процессор',\n",
       " 'цветок',\n",
       " 'рубашка',\n",
       " 'выпуск',\n",
       " 'серый',\n",
       " 'опт',\n",
       " 'остановка',\n",
       " 'пр',\n",
       " 'принимать',\n",
       " 'пластиковый',\n",
       " 'мина',\n",
       " 'мотор',\n",
       " 'контакт',\n",
       " 'демисезонный',\n",
       " 'широкий',\n",
       " 'внешний',\n",
       " 'подкладка',\n",
       " 'ссср',\n",
       " 'usb',\n",
       " 'водитель',\n",
       " '➡',\n",
       " 'игрушка',\n",
       " 'уровень',\n",
       " 'различный',\n",
       " 'серия',\n",
       " 'm',\n",
       " 'samsung',\n",
       " 'состав',\n",
       " 'спинка',\n",
       " 'старый',\n",
       " 'труба',\n",
       " 'внимание',\n",
       " 'ни',\n",
       " 'заказывать',\n",
       " 'программа',\n",
       " 'шиномонтаж',\n",
       " 'подбор',\n",
       " 'делать',\n",
       " 'верхний',\n",
       " 'человек',\n",
       " 'ручной',\n",
       " 'чемодан',\n",
       " 'отлично',\n",
       " 'просто',\n",
       " 'профессиональный',\n",
       " 'обеспечивать',\n",
       " '❗',\n",
       " 'код',\n",
       " 'специальный',\n",
       " 'н',\n",
       " 'неделя',\n",
       " 'выкуп',\n",
       " 'становиться',\n",
       " 'плитка',\n",
       " 'кнопка',\n",
       " 'ипотека',\n",
       " 'почти',\n",
       " 'прямо',\n",
       " 'видео',\n",
       " 'форд',\n",
       " '⭐',\n",
       " 'скорость',\n",
       " 'сдавать',\n",
       " 'рулевой',\n",
       " 'кто',\n",
       " 'a',\n",
       " 'оформление',\n",
       " 'ваз',\n",
       " 'дизайн',\n",
       " 'рассрочка',\n",
       " 'реальный',\n",
       " 'карман',\n",
       " '★',\n",
       " 'шаговый',\n",
       " 'интересовать',\n",
       " 'проверять',\n",
       " 'комфортный',\n",
       " 'италия',\n",
       " 'даже',\n",
       " 'l',\n",
       " 'матрас',\n",
       " 'in',\n",
       " 'объект',\n",
       " 'лицо',\n",
       " 'обогрев',\n",
       " 'зона',\n",
       " 'срочно',\n",
       " 'поддержка',\n",
       " 'элемент',\n",
       " 'пользоваться',\n",
       " 'сеть',\n",
       " 'pro',\n",
       " 'подробный',\n",
       " 'второй',\n",
       " 'поэтому',\n",
       " 'подвеска',\n",
       " 'страна',\n",
       " 'подключение',\n",
       " 'стена',\n",
       " 'игровой',\n",
       " 'собственный',\n",
       " 'сша',\n",
       " 'колонка',\n",
       " 'около',\n",
       " 'основной',\n",
       " 'кабель',\n",
       " 'насос',\n",
       " 'оперативный',\n",
       " 'толщина',\n",
       " 'ножка',\n",
       " 'фонарь',\n",
       " 'остальной',\n",
       " 'рад',\n",
       " 'покрытие',\n",
       " 'высылать',\n",
       " 'кресло',\n",
       " 'стильный',\n",
       " 'уместный',\n",
       " 'деньги',\n",
       " 'решетка',\n",
       " 'преимущество',\n",
       " 'функция',\n",
       " 'надежный',\n",
       " 'велосипед',\n",
       " 'крупный',\n",
       " 'осуществлять',\n",
       " 'консультация',\n",
       " 'подбирать',\n",
       " 'volkswagen',\n",
       " 'юридический',\n",
       " 'вход',\n",
       " 'гб',\n",
       " 'выбирать',\n",
       " 'защитный',\n",
       " 'практически',\n",
       " 'плата',\n",
       " 'московский',\n",
       " 'осмотр',\n",
       " 'mitsubishi',\n",
       " 'холодильник',\n",
       " 'масло',\n",
       " 'кроватка',\n",
       " 'двс',\n",
       " 'входить',\n",
       " 'сдэк',\n",
       " 'фильтр',\n",
       " 'парковка',\n",
       " 'class',\n",
       " 'торговый',\n",
       " 'отопление',\n",
       " 'вместе',\n",
       " 'дилер',\n",
       " 'r',\n",
       " '╸',\n",
       " 'направление',\n",
       " 'немного',\n",
       " 'длинный',\n",
       " 'металлический',\n",
       " 'пластик',\n",
       " 'нужно',\n",
       " 'прибор',\n",
       " 'зарядка',\n",
       " 'темно',\n",
       " 'момент',\n",
       " 'продукция',\n",
       " 'мес',\n",
       " 'эксплуатация',\n",
       " 'данные',\n",
       " 'мкад',\n",
       " 'просторный',\n",
       " 'стиральный',\n",
       " 'дорога',\n",
       " 'праздник',\n",
       " 'выход',\n",
       " 'др',\n",
       " 'сб',\n",
       " 'энергия',\n",
       " 'транспорт',\n",
       " 'инфраструктура',\n",
       " 'маркировка',\n",
       " 'тормозной',\n",
       " 'личный',\n",
       " 'балкон',\n",
       " 'ниссан',\n",
       " 'обслуживание',\n",
       " 'штаны',\n",
       " 'вс',\n",
       " 'договоренность',\n",
       " 'ростов',\n",
       " 'шерсть',\n",
       " 'число',\n",
       " 'аксессуар',\n",
       " 'разбор',\n",
       " 'e',\n",
       " 'пояс',\n",
       " 'дон',\n",
       " 'сапожок',\n",
       " 'мастер',\n",
       " 'дерево',\n",
       " '►',\n",
       " 'чек',\n",
       " 'жк',\n",
       " 'быстрый',\n",
       " 'стойка',\n",
       " 'плита',\n",
       " 'чем',\n",
       " 'осуществляться',\n",
       " 'нога',\n",
       " 'киа',\n",
       " 'написать',\n",
       " 'лето',\n",
       " 'отдельно',\n",
       " 'benz',\n",
       " 'инструмент',\n",
       " 'тц',\n",
       " 'электронный',\n",
       " 'санузел',\n",
       " 'птс',\n",
       " 'именно',\n",
       " 'зеленый',\n",
       " 'пт',\n",
       " 'царапина',\n",
       " 'приезжать',\n",
       " 'позвонить',\n",
       " 'электрический',\n",
       " 'течение',\n",
       " 'доставлять',\n",
       " 'воронеж',\n",
       " 'монитор',\n",
       " 'заводской',\n",
       " 'конверт',\n",
       " 'арт',\n",
       " 'авторазбор',\n",
       " 'способ',\n",
       " 'opel',\n",
       " 'тойота',\n",
       " 'яркий',\n",
       " 'возможно',\n",
       " 'возраст',\n",
       " 'подъезд',\n",
       " 'платеж',\n",
       " 'кофта',\n",
       " 'босоножка',\n",
       " 'приятный',\n",
       " 'покупаться',\n",
       " 'надевать',\n",
       " 'англия',\n",
       " 'ссылка',\n",
       " 'изготавливать',\n",
       " 'просьба',\n",
       " 'сборка',\n",
       " 'температура',\n",
       " 'комнатный',\n",
       " '�',\n",
       " 'комфорт',\n",
       " 'взрослый',\n",
       " 'h',\n",
       " 'станция',\n",
       " 'контрактный',\n",
       " 'кпп',\n",
       " 'бмв',\n",
       " 'последний',\n",
       " 'причина',\n",
       " 'положение',\n",
       " '🔥',\n",
       " 'переплата',\n",
       " 'идеально',\n",
       " 'mazda',\n",
       " 'сигнализация',\n",
       " 'строительство',\n",
       " 'амортизатор',\n",
       " 'газель',\n",
       " 'каблук',\n",
       " 'проживание',\n",
       " 'trade',\n",
       " '▂',\n",
       " 'родной',\n",
       " 'ждать',\n",
       " 'узнавать',\n",
       " 'курьер',\n",
       " 'раздел',\n",
       " 'нагрузка',\n",
       " 'нажимать',\n",
       " 'футболка',\n",
       " 'ящик',\n",
       " 'сервисный',\n",
       " 'лампа',\n",
       " 'розовый',\n",
       " 'шоссе',\n",
       " 'подогрев',\n",
       " 'средний',\n",
       " 'требоваться',\n",
       " 'снимать',\n",
       " 'd',\n",
       " 'тормоз',\n",
       " 'производить',\n",
       " 'фирменный',\n",
       " 'рама',\n",
       " 'ауди',\n",
       " 'дубленка',\n",
       " 'уютный',\n",
       " 'купля',\n",
       " 'поставка',\n",
       " '➖',\n",
       " 'всего',\n",
       " 'дисплей',\n",
       " 'сбербанк',\n",
       " 'примерно',\n",
       " 'встреча',\n",
       " 'стул',\n",
       " 'аналог',\n",
       " 'смс',\n",
       " 'требовать',\n",
       " 'жесткий',\n",
       " 'белгород',\n",
       " 'проезд',\n",
       " 'ряд',\n",
       " 'хотеть',\n",
       " 'золотой',\n",
       " 'доступный',\n",
       " 'рестайлинг',\n",
       " 'съемный',\n",
       " 'дача',\n",
       " 'fi',\n",
       " 'завод',\n",
       " 'многий',\n",
       " 'вт',\n",
       " 'дешево',\n",
       " 'гарантировать',\n",
       " 'стенка',\n",
       " 'замша',\n",
       " 'угол',\n",
       " 'германия',\n",
       " 'глубина',\n",
       " 'plus',\n",
       " 'стиль',\n",
       " 'специалист',\n",
       " 'предел',\n",
       " 'автоматический',\n",
       " 'подробность',\n",
       " 'диагностика',\n",
       " 'волгоград',\n",
       " 'приглашать',\n",
       " 'русский',\n",
       " 'проспект',\n",
       " 'проводить',\n",
       " 'искать',\n",
       " 'резинка',\n",
       " 'сообщать',\n",
       " 'oem',\n",
       " 'какой',\n",
       " 'шикарный',\n",
       " 'действовать',\n",
       " 'кольцо',\n",
       " 'изготовление',\n",
       " 'фольксваген',\n",
       " 'коврик',\n",
       " 'строительный',\n",
       " 'снг',\n",
       " 'кирпичный',\n",
       " 'топливный',\n",
       " 'машинка',\n",
       " 'соответствовать',\n",
       " 'копия',\n",
       " 'передача',\n",
       " 'желание',\n",
       " 'печка',\n",
       " 'свободный',\n",
       " 'точка',\n",
       " 'благодаря',\n",
       " 'смочь',\n",
       " 'проблема',\n",
       " 'прекрасный',\n",
       " 'свадебный',\n",
       " 'потертость',\n",
       " 'стр',\n",
       " '👍',\n",
       " 'примерка',\n",
       " 'зарядный',\n",
       " 'звук',\n",
       " 'входной',\n",
       " 'смотреться',\n",
       " 'таможенный',\n",
       " 'декларация',\n",
       " 'саратов',\n",
       " 'volvo',\n",
       " 'короткий',\n",
       " 'верх',\n",
       " 'коллекция',\n",
       " '×',\n",
       " 'осенний',\n",
       " 'владимир',\n",
       " 'великий',\n",
       " 'включать',\n",
       " 'автосервис',\n",
       " 'удобно',\n",
       " 'сутки',\n",
       " 'да',\n",
       " 'краска',\n",
       " 'учет',\n",
       " 'w',\n",
       " 'климат',\n",
       " 'сделка',\n",
       " 'утеплитель',\n",
       " 'отдых',\n",
       " 'брать',\n",
       " 'электропривод',\n",
       " '▼',\n",
       " 'формат',\n",
       " 'сдаваться',\n",
       " 'механизм',\n",
       " 'база',\n",
       " 'активный',\n",
       " 'камень',\n",
       " 'баня',\n",
       " 'смартфон',\n",
       " 'смоленск',\n",
       " 'тверь',\n",
       " 'газовый',\n",
       " 'габарит',\n",
       " 'плечо',\n",
       " 'пульт',\n",
       " 'целый',\n",
       " 'лоджия',\n",
       " 'intel',\n",
       " 'идти',\n",
       " 'аппарат',\n",
       " 'ход',\n",
       " 'регулироваться',\n",
       " 'деревянный',\n",
       " 'кухонный',\n",
       " 'генератор',\n",
       " 'скол',\n",
       " 'кабина',\n",
       " 'частота',\n",
       " '▒',\n",
       " 'sony',\n",
       " 'необходимость',\n",
       " 'язык',\n",
       " 'разъем',\n",
       " 'актуальный',\n",
       " 'дюйм',\n",
       " 'бензин',\n",
       " 'светлый',\n",
       " 'бизнес',\n",
       " 'воздух',\n",
       " 'решение',\n",
       " 'планировка',\n",
       " 'chevrolet',\n",
       " 'кит',\n",
       " 'встроенный',\n",
       " 'музыка',\n",
       " 'спб',\n",
       " 'видно',\n",
       " 'минимальный',\n",
       " 'брянск',\n",
       " 'рязань',\n",
       " 'сразу',\n",
       " 'рассматривать',\n",
       " 'планшет',\n",
       " 'предоставляться',\n",
       " 'лист',\n",
       " 'сотка',\n",
       " '°',\n",
       " 'g',\n",
       " 'шов',\n",
       " 'bluetooth',\n",
       " 'land',\n",
       " 'интересный',\n",
       " 'заменять',\n",
       " 'печь',\n",
       " 'wi',\n",
       " 'тестирование',\n",
       " 'ветровка',\n",
       " 'разрешение',\n",
       " 'спальный',\n",
       " 'средство',\n",
       " 'перекупщик',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list_lemmatized = sorted_tokens_lemmatized[:10000]\n",
    "tokens_list_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим функции `text_to_bow`, `descr_to_bow` и `title_to_bow` согласно лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text: str, tokens_list_lemmatized: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указано количество его употреблений в предложении\n",
    "    input: строка, список токенов\n",
    "    output: вектор той же размерности, что и список токенов\n",
    "    \"\"\"\n",
    "    # создаем вектор той же размерности, что и список токенов\n",
    "    vec = np.zeros(len(tokens_list_lemmatized))\n",
    "    for word in text:\n",
    "        if word in tokens_list_lemmatized:\n",
    "            # учитываем появление слова в векторе \n",
    "            vec[tokens_list_lemmatized.index(word)] += 1\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def descr_to_bow(items: np.array, tokens_list_lemmatized: list) -> np.array:\n",
    "    \"\"\" Для каждого описания товара возвращает вектор его bow \"\"\"\n",
    "    return np.array([text_to_bow(mystem_analyzer.lemmatize(description),\n",
    "                                 tokens_list_lemmatized) for title, description in tqdm(items)])\n",
    "\n",
    "def title_to_bow(items: np.array, tokens_list: list) -> np.array:\n",
    "    \"\"\" Для каждого заглавия товара возвращает вектор его bow \"\"\"\n",
    "    return np.array([text_to_bow(mystem_analyzer.lemmatize(title),\n",
    "                                 tokens_list_lemmatized) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:13<00:00, 284.27it/s]\n",
      "100%|██████████| 9000/9000 [00:34<00:00, 257.99it/s]\n",
      "100%|██████████| 21000/21000 [00:10<00:00, 2015.83it/s]\n",
      "100%|██████████| 9000/9000 [00:04<00:00, 2011.03it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_descr = descr_to_bow(X_train, tokens_list_lemmatized)\n",
    "X_test_descr = descr_to_bow(X_test, tokens_list_lemmatized)\n",
    "X_train_title = title_to_bow(X_train, tokens_list_lemmatized)\n",
    "X_test_title = title_to_bow(X_test, tokens_list_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложим два BOW-вектора, чтобы в итоге получить BOW-вектор одного объявления. Так же — как и раньше, — будем считать, что заголовок объявления в 2 раза важнее самого описания товара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = X_train_descr + 2 * X_train_title\n",
    "X_test_bow = X_test_descr + 2 * X_test_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также воспользуемся разреженными матрицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_csr = csr_matrix(X_train_bow)\n",
    "X_test_bow_csr = csr_matrix(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверем качество моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> a) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8054444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.3, random_state=13)\n",
    "lr.fit(X_train_bow_csr, y_train)\n",
    "y_pred = lr.predict(X_test_bow_csr)\n",
    "accuracy_4_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_4_logreg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Качество выросло. Теперь посмотрим на относительное увеличение качества при сравнении с первым вариантом алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с моделью без модификаций: 0.15%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с моделью без модификаций: {round((accuracy_4_logreg - accuracy_1_logreg) / accuracy_1_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> б) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем данные для SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled_csr = scaler.fit_transform(X_train_bow_csr)\n",
    "X_test_scaled_csr = scaler.transform(X_test_bow_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(C=0.3, random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_scaled_csr, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_scaled_csr)\n",
    "accuracy_4_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_4_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество выросло. Теперь посмотрим на относительное увеличение качества при сравнении с первым вариантом алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с моделью без модификаций: 0.16%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с моделью без модификаций: {round((accuracy_4_svm - accuracy_1_svm) / accuracy_1_svm, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: используя лемматизацию, наши модели улучшились, что и логично. Поскольку лемматизация — это процесс приведения разных форм одного слова к начальной форме, то есть лемме. В объявлении одно и то же слово может употребляться в разных наклонениях, падежах и т.д. — есть риск недосчитать важность какого-то релевантного слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF (5 баллов)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В `sklearn` есть `TfidfVectorizer`, но в этом задании его использовать нельзя. Для простоты посчитайте общий tf-idf для `'title'` и `'description'` (то есть каждому объекту надо сопоставить вектор, где как документ будет рассматриваться конкатенация `'title'` и `'description'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ составьте словарь, где каждому слову из изначального списка будет соответствовать количество документов из `train`-части, где это слово встретилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:01<00:00, 12065.21it/s]\n"
     ]
    }
   ],
   "source": [
    "word_document_cnt = {}\n",
    "for title, description in tqdm(X_train):\n",
    "    document = title + ' ' + description\n",
    "    # set() нужен для того, чтобы не учитывать больше одного раза слово,\n",
    "    # присутсвующее несколько раз в одном документе \n",
    "    for word in set(document.split()):\n",
    "         word_document_cnt[word] = word_document_cnt.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_document_cnt['размер'] == 2839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, где тексту в соответствие ставится tf-idf вектор. Для вычисления IDF также необходимо число документов в `train`-части (параметр `n_documents_total`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "    text = text.split()\n",
    "    vec = np.zeros(len(tokens_list))\n",
    "    \n",
    "    # пробегаем по каждому слову в строке\n",
    "    for term in set(text):\n",
    "        # если это слово есть в нашем топ-10000 списке, считаем для него TF-IDF\n",
    "        if term in tokens_list:\n",
    "            TF_t = text.count(term) / len(set(text))\n",
    "            IDF_t = np.log(n_documents_total / word_document_cnt[term])\n",
    "            # присваеваем индексу вектора — конкретному слову в этом векторе — значение TF-IDF\n",
    "            vec[tokens_list.index(term)] = TF_t * IDF_t\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = text_to_tfidf(\n",
    "    'сдаётся уютный , тёплый гараж для стартапов в ml',\n",
    "    word_document_cnt,\n",
    "    tokens_list,\n",
    "    n_documents_total=len(X_train)\n",
    ")\n",
    "assert 0.0003 < example_text.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого объекта сопоставляет вектор tf-idf. В качестве текстов используйте конкатенацию `'title'` и `'description'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_tfidf(items: np.array, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Для каждого товара возвращает его tf-idf вектор\n",
    "    \"\"\"\n",
    "      \n",
    "    return np.array([text_to_tfidf(title + \" \" + description,\n",
    "                                   word_document_cnt,\n",
    "                                   tokens_list,\n",
    "                                   n_documents_total) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:24<00:00, 247.75it/s]\n",
      "100%|██████████| 9000/9000 [00:37<00:00, 238.52it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = items_to_tfidf(X_train, word_document_cnt, tokens_list, len(X_train))\n",
    "X_test_tfidf = items_to_tfidf(X_test, word_document_cnt, tokens_list, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_tfidf.shape == (21000, 10000), X_test_tfidf.shape == (9000, 10000)\n",
    "assert 0.0002 < X_train_tfidf.mean() < 0.0004\n",
    "assert 0.0002 < X_test_tfidf.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся разреженными матрицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_csr = csr_matrix(X_train_tfidf)\n",
    "X_test_tfidf_csr = csr_matrix(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "__Задание:__ обучите логистическую регрессию и SVC, оцените качество (accuracy_score). Сделайте вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> a) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7311111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13)\n",
    "lr.fit(X_train_tfidf_csr, y_train)\n",
    "y_pred = lr.predict(X_test_tfidf_csr)\n",
    "accuracy_5_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_5_logreg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [],
   "source": [
    "assert accuracy_score(y_test, y_pred) > 0.675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с первой моделью: 0.04%\n",
      "Относительное ухудшение при сравнении с четвертой моделью (bow-вектора и лемматизация): -0.09%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с первой моделью: {round((accuracy_5_logreg - accuracy_1_logreg) / accuracy_1_logreg, 2)}%\")\n",
    "print(f\"Относительное ухудшение при сравнении с четвертой моделью (bow-вектора и лемматизация): {round((accuracy_5_logreg - accuracy_4_logreg) / accuracy_4_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> б) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7923333333333333\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(random_state=13, max_iter=200)\n",
    "SVM_linear_classifier.fit(X_train_scaled_csr, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_scaled_csr)\n",
    "accuracy_5_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_5_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert accuracy_score(y_test, y_pred) > 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное улучшение при сравнении с первой моделью: 0.16%\n",
      "Относительное ухудшение при сравнении с четвертой моделью (bow-вектора и лемматизация): -0.00335%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное улучшение при сравнении с первой моделью: {round((accuracy_5_svm - accuracy_1_svm) / accuracy_1_svm, 2)}%\")\n",
    "print(f\"Относительное ухудшение при сравнении с четвертой моделью (bow-вектора и лемматизация): {round((accuracy_5_svm - accuracy_4_svm) / accuracy_4_svm, 5)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: bow-вектора в сочетании с лемматизацией лучше TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (4 балла)\n",
    "\n",
    "Давайте попробуем другой подход — каждому слову сопоставим какое-то векторное представление (эмбеддинг) — но достаточно маленькой размерности. Таким образом мы сильно уменьшим количество параметров в модели.\n",
    "\n",
    "Почитать про это подробнее можно тут:\n",
    "\n",
    "- https://habr.com/ru/company/ods/blog/329410/\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах из интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "T38J27NcYGx5",
    "outputId": "57fa3a9f-13a3-4fa1-d13c-3c0c49a86a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-14 19:46:14--  https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/0x7oxso6x93efzj/ru.tar.gz [following]\n",
      "--2020-04-14 19:46:15--  https://www.dropbox.com/s/raw/0x7oxso6x93efzj/ru.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com/cd/0/inline/A10A5d72axWk8fNGTC-8opH-bFupom45H37XWfyerZLZLiFxnXE-FTmViuHMUVOtlKIUlBqcoxvjmX2bVvSLuU0qKpDJOeEaKoE3GZH3Y1Vgtw/file# [following]\n",
      "--2020-04-14 19:46:15--  https://uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com/cd/0/inline/A10A5d72axWk8fNGTC-8opH-bFupom45H37XWfyerZLZLiFxnXE-FTmViuHMUVOtlKIUlBqcoxvjmX2bVvSLuU0qKpDJOeEaKoE3GZH3Y1Vgtw/file\n",
      "Resolving uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com (uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com)... 162.125.70.6\n",
      "Connecting to uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com (uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com)|162.125.70.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/A13_L7BFNhfyYwhz3_voQhqYaUoaOhJxBw7F_J7N63W6tj4QUPEhzD67RAsIFcUghzuOncsKUEmYNtjvqjiVM96vOrh7lPRDTcdkdfYSgCKmlSB6DFzdlN8WSAA24kk4HBNcMCUdOLgejZvJh4maLOOphfuCH5CurK37_ybGw25WBepYIDrgRQvhZ-eKvP9KDVyeL_ypC6feBaSz4pGM2tAl3j-Yzk_QevL8ryMY_ctYn0fqtPcI3JSzwinhQlzlSQFDcXwt6btJd3STmbccG0SW657vt9Zfp-XIg11hS4RneVN46lqgSrp_z_JJOhLg7_OoThQYKgADeA-87POSa16m/file [following]\n",
      "--2020-04-14 19:46:16--  https://uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com/cd/0/inline2/A13_L7BFNhfyYwhz3_voQhqYaUoaOhJxBw7F_J7N63W6tj4QUPEhzD67RAsIFcUghzuOncsKUEmYNtjvqjiVM96vOrh7lPRDTcdkdfYSgCKmlSB6DFzdlN8WSAA24kk4HBNcMCUdOLgejZvJh4maLOOphfuCH5CurK37_ybGw25WBepYIDrgRQvhZ-eKvP9KDVyeL_ypC6feBaSz4pGM2tAl3j-Yzk_QevL8ryMY_ctYn0fqtPcI3JSzwinhQlzlSQFDcXwt6btJd3STmbccG0SW657vt9Zfp-XIg11hS4RneVN46lqgSrp_z_JJOhLg7_OoThQYKgADeA-87POSa16m/file\n",
      "Reusing existing connection to uc9c5161deffa507fea87f11afd8.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2399456034 (2.2G) [application/octet-stream]\n",
      "Saving to: ‘ru.tar.gz.1’\n",
      "\n",
      "ru.tar.gz.1         100%[===================>]   2.23G  6.13MB/s    in 10m 3s  \n",
      "\n",
      "2020-04-14 19:56:21 (3.79 MB/s) - ‘ru.tar.gz.1’ saved [2399456034/2399456034]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfse4xVbgMIr"
   },
   "outputs": [],
   "source": [
    "!tar -xzf ru.tar.gz.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.2-cp37-cp37m-macosx_10_9_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-1.11.1.tar.gz (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.39-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.39\n",
      "  Downloading botocore-1.15.39-py2.py3-none-any.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 2.9 MB/s eta 0:00:01     |█████████████████████▏          | 4.0 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.11.1-py3-none-any.whl size=95255 sha256=c6e6a57281bf6319974e29683d08b3589212c26aad23865c80a2769cb15feb45\n",
      "  Stored in directory: /Users/khaykingleb/Library/Caches/pip/wheels/1a/8c/a2/7b24df77c58dab0aec275c1bd3d4392d54df941020199f6759\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.12.39 botocore-1.15.39 docutils-0.15.2 gensim-3.8.2 jmespath-0.9.5 s3transfer-0.3.3 smart-open-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "embedding_model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.02916384  0.02167605  0.05127367 -0.00971958  0.0465235  -0.03945766\n",
      "  0.02737866  0.00638128 -0.03774629 -0.04257201 -0.00995653  0.02291315\n",
      " -0.02301722  0.06697998 -0.03674482 -0.02403202 -0.05404469  0.01372932\n",
      "  0.00926399 -0.0013149   0.11941359 -0.022448    0.04011497  0.06980549\n",
      "  0.00407011 -0.09384539  0.03050164 -0.02578281 -0.03525181 -0.06603175\n",
      "  0.04752798  0.05874675  0.01983666  0.06092105 -0.00957561  0.08307806\n",
      " -0.01288903  0.04705157  0.02198839 -0.00649013 -0.0171444   0.03302203\n",
      "  0.02124882 -0.01902875 -0.05235172  0.03458685 -0.01409259 -0.07477519\n",
      "  0.01916078  0.02985001  0.0086322   0.03051201  0.02831862  0.04549561\n",
      "  0.00761138 -0.05459622  0.09056009 -0.08807947 -0.05420396 -0.04793203\n",
      " -0.05672329 -0.03025264 -0.03024072 -0.05890108 -0.03137474  0.03292617\n",
      "  0.05440779 -0.04548327 -0.07266086 -0.09327219  0.07247883  0.0111061\n",
      "  0.01824225 -0.10570452  0.05110046 -0.04659343 -0.03277056 -0.00803401\n",
      " -0.03978698  0.00826598 -0.01074128  0.018431   -0.10150263 -0.00472604\n",
      "  0.06706332  0.02466901  0.09045192 -0.05226929  0.04866098 -0.02843297\n",
      "  0.04756537  0.00261342  0.06845197  0.00082511 -0.00547984  0.0100649\n",
      "  0.02135489 -0.01437242  0.00191435  0.11989547  0.02357679  0.07061605\n",
      "  0.03375214  0.05462346  0.08270866  0.00126649  0.03054527  0.04314573\n",
      " -0.00719835 -0.02799017  0.00249404  0.00139046 -0.04099929  0.00526204\n",
      "  0.01386764  0.02106066  0.00887202  0.05943111 -0.07185322  0.03263306\n",
      "  0.00284878  0.03816929  0.0210096  -0.030828    0.00502779  0.09250114\n",
      "  0.02399154  0.05744717 -0.04319151  0.04075926 -0.03877947  0.0605263\n",
      " -0.00837917 -0.04922852 -0.04570796  0.02973622 -0.01798053  0.00413011\n",
      " -0.00712464 -0.01312802  0.05847022 -0.07881333 -0.02204878  0.03086594\n",
      "  0.02965177 -0.0073295  -0.02443145 -0.06222062  0.01083152  0.06009534\n",
      " -0.02042049  0.06301811  0.02287635 -0.03021961  0.04831248  0.02882019\n",
      "  0.04446645 -0.01677353 -0.08272323 -0.06830658  0.08947854  0.03370909\n",
      " -0.00895046 -0.00681254 -0.02059644 -0.09527113  0.02611189 -0.06112244\n",
      "  0.01080315  0.01901113  0.00810233  0.00742132  0.10493557 -0.00522375\n",
      "  0.05826566  0.03236291  0.03787734 -0.05026894 -0.08401242  0.02860721\n",
      " -0.05106218  0.02631241  0.02631763  0.06924202  0.03319636  0.00980412\n",
      "  0.04016861  0.03428936  0.00652957 -0.01058654 -0.0245588   0.1464914\n",
      " -0.01041028  0.03553488 -0.07482928 -0.01063148 -0.0342233  -0.01662586\n",
      " -0.00029508  0.04694034 -0.00062491 -0.0435293  -0.01315623  0.07061336\n",
      "  0.01603698  0.02374655  0.05453315  0.00253603 -0.0313729  -0.02740866\n",
      "  0.04278845 -0.00810288  0.03973977  0.07674816  0.04658518 -0.02685211\n",
      " -0.05009724  0.0060723  -0.04231661  0.02584185 -0.03419575 -0.03799306\n",
      "  0.06701688 -0.1245426   0.03846397 -0.0855662  -0.01193651  0.04968415\n",
      "  0.03559558  0.10029506  0.05714916  0.01145345 -0.03564315 -0.00924199\n",
      "  0.08630151  0.08049053  0.05822275 -0.05224873 -0.02462301  0.05832206\n",
      " -0.04124978  0.00186134  0.00782246  0.01179015 -0.02291097  0.00614069\n",
      "  0.01782681  0.02190027  0.04341367  0.06151633 -0.01183114 -0.00141502\n",
      "  0.06193598  0.0611085  -0.02373199 -0.05797793 -0.02269631  0.11511736\n",
      " -0.04581353 -0.05082048 -0.04706197  0.0429772   0.00409648 -0.0141248\n",
      "  0.01417164  0.00575812 -0.07616108 -0.01051838  0.05149659  0.02367133\n",
      "  0.00073724  0.05957585 -0.11871962  0.03876314  0.03472188 -0.02344368\n",
      " -0.01165281 -0.01397923  0.08815268  0.03459521  0.07113555 -0.03984846\n",
      " -0.01600395  0.01932258  0.01351069 -0.06409036 -0.02024848  0.05895981\n",
      "  0.02591374 -0.04027611  0.00654722  0.05093394 -0.02461737  0.02561689\n",
      " -0.01412898 -0.00366109 -0.06719207  0.00742674 -0.02095614 -0.06263787]\n"
     ]
    }
   ],
   "source": [
    "# как мы видим, каждому слову данная модель сопоставляет вектор размерности 300\n",
    "print(embedding_model['привет'].shape)\n",
    "print(embedding_model['привет'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, выдающую эмбеддинг для предложения — как сумму эмбеддингов токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, embedding_model) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    sent_embed = np.zeros(300)\n",
    "    # поскольку для знаков пунктуации и нерусских слов нету векторного представления в embedding_model,\n",
    "    # нам необходимо \"почистить\" наш текст посредством регулярных выражений\n",
    "    prog = re.compile('[А-Яа-яё\\-]+')\n",
    "    \n",
    "    for word in prog.findall(sentence):\n",
    "        try:\n",
    "            sent_embed += embedding_model[word]\n",
    "        except KeyError:\n",
    "            pass # Исключаем иные слова (наподобие зп), которые также не имеют векторного представления\n",
    "    \n",
    "    return sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model).shape == (300,)\n",
    "assert np.allclose(np.linalg.norm(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model)), 2.6764746)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ сделайте все то же, что в предыдущих пунктах — реализуйте функцию, которая преобразует данные, а затем обучите логистическую регрессию и SVM, оцените качество. Сделайте вывод, что работает лучше: модель, основанная на TF-IDF, или модель, обученная на предобученных эмбеддингах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>1. Реализуем функцию, которая преобразует данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_embedding(items: np.array, embedding_model) -> np.array:\n",
    "    return np.array([sentence_embedding(title + \" \" + description,\n",
    "                                        embedding_model) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:20<00:00, 1029.92it/s]\n",
      "100%|██████████| 9000/9000 [00:09<00:00, 999.27it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train_embed = items_to_embedding(X_train, embedding_model)\n",
    "X_test_embed = items_to_embedding(X_test, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(csr_matrix(X_train_embed))\n",
    "X_test_scaled = scaler.transform(csr_matrix(X_test_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> a) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4201111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "accuracy_6_logreg = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_6_logreg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное ухудшение при сравнении с пятой моделью (TF-IDF): -0.43%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное ухудшение при сравнении с пятой моделью (TF-IDF): {round((accuracy_6_logreg - accuracy_5_logreg) / accuracy_5_logreg, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> б) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5164444444444445\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(random_state=13, max_iter=200)\n",
    "SVM_linear_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_scaled)\n",
    "accuracy_6_svm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_6_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительное ухудшение при сравнении с пятой моделью (TF-IDF): -0.35%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Относительное ухудшение при сравнении с пятой моделью (TF-IDF): {round((accuracy_6_svm - accuracy_5_svm) / accuracy_5_svm, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: При сравнении с моделью, основанной на TF-IDF, модель, обученная на предобученных эмбеддингах, явно уступает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше? (8 баллов)\n",
    "\n",
    "Для получения максимальной оценки вам нужно решить любые 2 пункта. Решение каждого пункта даст вам полтора балла:\n",
    "\n",
    "1. Реализовать n-gram модели текстовой классификации (__2 балла__)\n",
    "\n",
    "2. Поработать с другими эмбеддингами для слов (например `word2vec` или `GloVe`) (__2 балла__)\n",
    "\n",
    "3. Применить другие способы токенизации (например, `pymorphy2`, `spaCy`) и в целом предобработки данных (стоп-слова, стэмминг, лемматизация) (__2 балла__)\n",
    "\n",
    "4. Добиться качества > 0.81 на тестовых данных (попробуйте другие токенизаторы, предобработку текста, и любые другие идеи, которые вам придут в голову) (__1 балл__)\n",
    "\n",
    "4. Добиться качества > 0.82 на тестовых данных (попробуйте другие токенизаторы, предобработку текста, и любые другие идеи, которые вам придут в голову) (__1 балл__)\n",
    "\n",
    "Снабжайте код пояснениями и графиками.\n",
    "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>I. Удалим ненужные слова — воспользуемся стоп-словами — и пунктуацию в объявлениях (__третий пункт__) и снова лемматизируем наш текст. Также реализуем n-gram модель текстовой классификации (__первый пункт__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem(entire_input=False)\n",
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_noise(text: list, noise: list):\n",
    "    return \" \".join([token.lower() for token in text if token not in noise and len(token) > 2])\n",
    "\n",
    "def preprocess(X, noise: list):\n",
    "    return np.array([del_noise(mystem_analyzer.lemmatize(title), noise) \n",
    "                     + \" \" +\n",
    "                     del_noise(mystem_analyzer.lemmatize(description), noise) for title, description in tqdm(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:11<00:00, 295.10it/s]\n",
      "100%|██████████| 9000/9000 [00:30<00:00, 291.59it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_preproc = preprocess(X_train, noise)\n",
    "X_test_preproc = preprocess(X_test, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь и оставим только топ-20000 самых частых слов, а также создам отдельный список из этих слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_cnt = dict()\n",
    "for text in X_train_preproc:\n",
    "    for term in text.split():\n",
    "         tokens_cnt[term] = tokens_cnt.get(term, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(tokens_cnt, key=lambda x: tokens_cnt.get(x), reverse=True)\n",
    "tokens_list = sorted_tokens[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем n-gram модель текстовой классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем n-граммы\n",
    "tfidf_vec_ngram = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), vocabulary=tokens_list)\n",
    "X_train_vec = tfidf_vec_ngram.fit_transform(X_train_preproc)\n",
    "X_test_vec = tfidf_vec_ngram.transform(X_test_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7828888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13)\n",
    "lr.fit(csr_matrix(X_train_vec), y_train)\n",
    "y_pred = lr.predict(csr_matrix(X_test_vec))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "X_train_vec_scaled = csr_matrix(scaler.fit_transform(X_train_vec))\n",
    "X_test_vec_scaled = csr_matrix(scaler.transform(X_test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203333333333334\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(C=0.145, random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_vec_scaled, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_vec_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вывод__: комбинация стоп-слов, лемматизации, выброса пунктуации и слов длинной в 2 символа с TF-IDF векторами в сочетании дала неплохое качество. Теперь попробуем его дальше улучшить.\n",
    "\n",
    "Так же мы достигли качества в 0.82 и, тем самым, выполнили __четырый__ и __пятый__ пункты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>II. Поработаем с эмбеддингом `Word2Vec` (__второй пункт__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим `X_train_lemmatized` и `X_test_lemmatized` как лист листов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = [text.split() for text in X_train_preproc]\n",
    "X_test_new = [text.split() for text in X_test_preproc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train_new, size=200, min_count=1, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('apple', 0.9374587535858154),\n",
       " ('gray', 0.9266039729118347),\n",
       " ('samsung', 0.9129846096038818),\n",
       " ('galaxy', 0.9117615818977356),\n",
       " ('ростест', 0.9086523056030273),\n",
       " ('silicone', 0.9059789180755615),\n",
       " ('plus', 0.9035242199897766),\n",
       " ('xiaomi', 0.8967676162719727),\n",
       " ('отвязывать', 0.8921439051628113),\n",
       " ('min', 0.888987123966217)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar('iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word2vec_ndim = model['iphone'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы обучили модель `Word2Vec` для нашей обучающей выборки. Теперь реализуем ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, embedding_model) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    sent_embed = np.zeros(word2vec_ndim)\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            sent_embed += embedding_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return sent_embed\n",
    "\n",
    "def items_to_embedding(items: np.array, embedding_model) -> np.array:\n",
    "    return np.array([sentence_embedding(title + \" \" + description,\n",
    "                                        embedding_model) for title, description in tqdm(items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21000 [00:00<?, ?it/s]/opt/anaconda3/envs/r_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 21000/21000 [00:07<00:00, 2900.20it/s]\n",
      "100%|██████████| 9000/9000 [00:02<00:00, 3064.77it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_embed = items_to_embedding(X_train, model)\n",
    "X_test_embed = items_to_embedding(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_crs = csr_matrix(scaler.fit_transform(X_train_embed))\n",
    "X_test_crs = csr_matrix(scaler.transform(X_test_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5157777777777778\n"
     ]
    }
   ],
   "source": [
    "SVM_linear_classifier = LinearSVC(C=0.145, random_state=13)\n",
    "SVM_linear_classifier.fit(X_train_crs, y_train)\n",
    "y_pred = SVM_linear_classifier.predict(X_test_crs)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вывод__: Даже обученный на наших данных, а не готовый, эмбеддинг не показал хороших результатов. Возможно, это следствие того, что корпус текстов относительно мал."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
